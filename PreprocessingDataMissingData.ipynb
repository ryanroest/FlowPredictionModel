{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# for the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, RepeatVector, TimeDistributed, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[554.1805633333325, 0.0, 0.0, ..., 0.0, 1.0, 1.0],\n       [512.610779583333, 1.0, 0.0, ..., 0.0, 0.0, 1.0],\n       [495.14290208333324, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n       ...,\n       [113.38789088512496, 0.0, 0.0, ..., 0.0, 0.0, 1.0],\n       [77.43711137416668, 0.0, 0.0, ..., 0.0, 0.0, 1.0],\n       [163.3501073891665, 0.0, 0.0, ..., 0.0, 0.0, 1.0]], dtype=object)"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load files from pre-processing\n",
    "\n",
    "#X = np.load('X.npy') #rainpredictions, hour of day, month of year, holiday, ones\n",
    "#y = np.load('y.npy')\n",
    "\n",
    "X_dry = np.load('X_dry.npy',allow_pickle=True)\n",
    "y_dry = np.load('Y_dry.npy',allow_pickle=True)\n",
    "X_rain = np.load('X_rain.npy',allow_pickle=True)\n",
    "y_rain = np.load('Y_rain.npy',allow_pickle=True)\n",
    "TimeHour = pd.read_csv('Timehour_wet.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "# split dataset in test and train\n",
    "\n",
    "def ReshapeXData(dataframe):\n",
    "    n_hours = 24\n",
    "    n_features = dataframe.shape[1]\n",
    "    dataframe_shaped = np.reshape(dataframe,(-1, n_hours, n_features))\n",
    "    return dataframe_shaped\n",
    "\n",
    "def ReshapeYData(dataframe):\n",
    "    n_hours = 24\n",
    "    n_features = 1\n",
    "    dataframe_shaped = np.reshape(dataframe,(-1, n_hours, n_features))\n",
    "    return dataframe_shaped\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rain, y_rain, test_size=0.19847328244, random_state=42)\n",
    "\n",
    "X_shaped = ReshapeXData(X_rain)\n",
    "X_train = ReshapeXData(X_train)\n",
    "X_test = ReshapeXData(X_test)\n",
    "y_train = ReshapeYData(y_train)\n",
    "y_test = ReshapeYData(y_test)\n",
    "#X_train_gru = X[0:10800,:]\n",
    "#X_test_gru = X[10801:13609,:]\n",
    "#Y_train_gru = y[0:10800]\n",
    "#Y_test_gru = y[10801:13609]\n",
    "\n",
    "\n",
    "#X_train_gru_shaped=ReshapeXData(X_train_gru)\n",
    "#X_test_gru_shaped = ReshapeXData(X_test_gru)\n",
    "#Y_train_gru_shaped = ReshapeYData(Y_train_gru)\n",
    "#Y_test_gru_shaped = ReshapeYData(Y_test_gru)\n",
    "#X_shaped = ReshapeXData(X[0:13608,:])\n",
    "#Y_shaped = ReshapeYData(y[0:13608])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 84 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 223.3351 - val_loss: 221.5797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 217.0239 - val_loss: 214.7480\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 210.3318 - val_loss: 207.3567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 202.9140 - val_loss: 199.8026\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 195.2464 - val_loss: 191.9962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 187.2317 - val_loss: 184.3537\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 179.2071 - val_loss: 178.0188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 172.7125 - val_loss: 175.5251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 170.4136 - val_loss: 176.4542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 171.0348 - val_loss: 176.7796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 170.8741 - val_loss: 175.6461\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 169.7071 - val_loss: 174.8297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 169.0332 - val_loss: 174.6225\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.9776 - val_loss: 174.5739\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.8038 - val_loss: 174.3607\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.5212 - val_loss: 174.0897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.2032 - val_loss: 173.8306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.9362 - val_loss: 173.6306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.7348 - val_loss: 173.4749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.6074 - val_loss: 173.3372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.4366 - val_loss: 173.2015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.2591 - val_loss: 173.1130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.1071 - val_loss: 173.0374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.9603 - val_loss: 173.0141\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.8000 - val_loss: 172.9311\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.6145 - val_loss: 172.8193\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.4459 - val_loss: 172.7142\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.3318 - val_loss: 172.6478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.1678 - val_loss: 172.6277\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.0352 - val_loss: 172.6797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.9487 - val_loss: 172.6532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.7711 - val_loss: 172.6207\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.6338 - val_loss: 172.5414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.5360 - val_loss: 172.4926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.4098 - val_loss: 172.4935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.2657 - val_loss: 172.5226\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.1198 - val_loss: 172.5574\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.9645 - val_loss: 172.7256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.8812 - val_loss: 172.5074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.7918 - val_loss: 172.4765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.6877 - val_loss: 172.7204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.4980 - val_loss: 172.6594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3139 - val_loss: 172.8372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.2705 - val_loss: 172.9310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.2099 - val_loss: 172.8990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.9960 - val_loss: 173.4894\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.8762 - val_loss: 173.3930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.6579 - val_loss: 173.8405\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.3871 - val_loss: 173.9036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.5809 - val_loss: 173.9409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.3425 - val_loss: 173.8626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.2287 - val_loss: 173.8990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.0996 - val_loss: 174.0051\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 163.0102 - val_loss: 174.1531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.9499 - val_loss: 174.2281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.8603 - val_loss: 174.1967\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.7782 - val_loss: 174.1300\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.7094 - val_loss: 174.2015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.6211 - val_loss: 174.3953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.5318 - val_loss: 174.1568\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.4394 - val_loss: 174.1339\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.4148 - val_loss: 174.1218\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.3920 - val_loss: 174.1158\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.3528 - val_loss: 174.1192\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.3283 - val_loss: 174.1314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.3018 - val_loss: 174.1409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2841 - val_loss: 174.1506\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2674 - val_loss: 174.1580\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2501 - val_loss: 174.1705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2369 - val_loss: 174.1807\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.9999999949504855e-07.\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2252 - val_loss: 174.1817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2238 - val_loss: 174.1828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2221 - val_loss: 174.1838\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2207 - val_loss: 174.1852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2190 - val_loss: 174.1859\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2173 - val_loss: 174.1863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2155 - val_loss: 174.1864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2143 - val_loss: 174.1873\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2125 - val_loss: 174.1882\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2109 - val_loss: 174.1891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 2.000000023372195e-08.\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2099 - val_loss: 174.1892\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2097 - val_loss: 174.1894\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2095 - val_loss: 174.1895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2094 - val_loss: 174.1897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2092 - val_loss: 174.1897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2090 - val_loss: 174.1899\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2088 - val_loss: 174.1900\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2087 - val_loss: 174.1901\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2085 - val_loss: 174.1902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2084 - val_loss: 174.1903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.9999999878450583e-09.\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2083 - val_loss: 174.1903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2083 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2083 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 162.2082 - val_loss: 174.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.999999943436137e-10.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1678696ec88>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic model\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "lr=0.0002\n",
    "epochs=100\n",
    "batch_size=32\n",
    "validation_split=0.1\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "#n_timesteps, n_features, n_outputs = X_train_gru.shape[0], X_train_gru.shape[1], Y_train_gru.shape[0]\n",
    "# reshape output into [samples, timesteps, features]\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Reshape((1, X_train_gru.shape[1])))\n",
    "model.add(GRU(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "#model.add(GRU(200, activation='tanh', input_shape=(1,X_train_gru.shape[1])))\n",
    "model.add(RepeatVector(n_outputs))\n",
    "model.add(GRU(200, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "opt=optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='mae', optimizer=opt)\n",
    "\n",
    "model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            ReduceLROnPlateau(\n",
    "                verbose=1)])\n",
    "    \n",
    "    \n",
    "#model = Sequential()\n",
    "#model.add(Dense(1, input_dim=X.shape[1]))\n",
    "#model.compile(optimizer = optimizers.Adam(lr=lr), loss='mean_squared_error')\n",
    "#model.fit(X_train, y_train,\n",
    "#          validation_split=validation_split,\n",
    "#          epochs=epochs,\n",
    "#          batch_size=batch_size,\n",
    "#          shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# make a prediction to test how well the model preforms\n",
    "y_predicted = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### LZS: WE HAVE TO IMPLEMENT HERE SOME METRICS AND A LOT OF PLOTS SO THAT WE CAN SEE HOW WELL THE MODEL PERFORMS\n",
    "### LZS: AND ALSO TO BE ABLE TO FIND NEW FEATURES TO IMPROVE THE MODEL\n",
    "### LZS: AND TO EVALUATE IF THE MODEL INDEED IMPROVES WHEN ADDING MORE FEATURES/ OTHER LAYERS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted=[72.248436], Original=[84.4234122904167]\n",
      "Predicted=[86.923], Original=[862.5090558333333]\n",
      "Predicted=[88.00898], Original=[365.5664364264861]\n",
      "Predicted=[88.88442], Original=[336.159522455139]\n",
      "Predicted=[90.23452], Original=[146.23547872013904]\n",
      "Predicted=[91.5743], Original=[63.33341904555554]\n",
      "Predicted=[92.92094], Original=[498.6270929166664]\n",
      "Predicted=[94.30749], Original=[514.8907244444447]\n",
      "Predicted=[95.76746], Original=[513.2867190277781]\n",
      "Predicted=[97.24717], Original=[43.21729329622221]\n",
      "Predicted=[98.77364], Original=[35.78967036840332]\n",
      "Predicted=[100.38198], Original=[120.85732251188983]\n",
      "Predicted=[102.02639], Original=[35.80649369249831]\n",
      "Predicted=[103.73035], Original=[10.226580554208333]\n",
      "Predicted=[105.49932], Original=[91.94390274458337]\n",
      "Predicted=[107.35139], Original=[83.82949924069443]\n",
      "Predicted=[109.34038], Original=[104.96816865428814]\n",
      "Predicted=[111.46469], Original=[496.9309556388885]\n",
      "Predicted=[113.796715], Original=[151.64836002888876]\n",
      "Predicted=[116.36996], Original=[95.65714740375]\n",
      "Predicted=[119.10248], Original=[93.86623877569446]\n",
      "Predicted=[121.908165], Original=[94.59925739417781]\n",
      "Predicted=[124.78206], Original=[44.63301945527779]\n",
      "Predicted=[127.647606], Original=[514.359438333333]\n",
      "Predicted=[91.45103], Original=[695.5614265277777]\n",
      "Predicted=[144.75455], Original=[248.1164779212142]\n",
      "Predicted=[168.10944], Original=[312.5038614334025]\n",
      "Predicted=[179.6181], Original=[91.9490770348611]\n",
      "Predicted=[190.17851], Original=[895.3606036111117]\n",
      "Predicted=[191.5315], Original=[510.59596483333326]\n",
      "Predicted=[193.64905], Original=[44.677509444444446]\n",
      "Predicted=[198.29056], Original=[79.85918841194444]\n",
      "Predicted=[203.90259], Original=[88.93424712666663]\n",
      "Predicted=[209.03241], Original=[514.6614563888891]\n",
      "Predicted=[213.46404], Original=[93.41037911902775]\n",
      "Predicted=[217.20248], Original=[893.2569612499996]\n",
      "Predicted=[220.41351], Original=[42.46612747416665]\n",
      "Predicted=[223.29324], Original=[58.879927532222254]\n",
      "Predicted=[226.0008], Original=[514.6648823611116]\n",
      "Predicted=[228.64017], Original=[580.1238087499999]\n",
      "Predicted=[231.29965], Original=[515.9422194444447]\n",
      "Predicted=[234.01785], Original=[103.96548232012216]\n",
      "Predicted=[236.72702], Original=[146.7361828126389]\n",
      "Predicted=[239.2027], Original=[895.8721854166669]\n",
      "Predicted=[241.28497], Original=[35.82338299041328]\n",
      "Predicted=[242.94058], Original=[386.4051141388889]\n",
      "Predicted=[244.2438], Original=[145.55625737295844]\n",
      "Predicted=[245.29393], Original=[515.0571497222222]\n",
      "Predicted=[74.41932], Original=[496.56238333333357]\n",
      "Predicted=[92.42312], Original=[78.19499146430553]\n",
      "Predicted=[107.11201], Original=[505.1233406944443]\n",
      "Predicted=[112.572815], Original=[44.43692492704645]\n",
      "Predicted=[116.08143], Original=[35.85716158624321]\n",
      "Predicted=[119.81515], Original=[495.7129206944445]\n",
      "Predicted=[123.6515], Original=[42.153919867083324]\n",
      "Predicted=[127.57996], Original=[507.92877875000016]\n",
      "Predicted=[131.6651], Original=[460.25736093570816]\n",
      "Predicted=[135.77269], Original=[510.7527401944444]\n",
      "Predicted=[139.80707], Original=[35.79013218514318]\n",
      "Predicted=[143.60843], Original=[35.79804904354083]\n",
      "Predicted=[147.49626], Original=[174.14673291399293]\n",
      "Predicted=[151.63913], Original=[510.08210375000004]\n",
      "Predicted=[156.96167], Original=[91.51977398482158]\n",
      "Predicted=[162.50583], Original=[144.79848177791655]\n",
      "Predicted=[166.53372], Original=[73.33331787041666]\n",
      "Predicted=[170.70584], Original=[366.25058951611067]\n",
      "Predicted=[177.44983], Original=[122.14905630930562]\n",
      "Predicted=[184.42822], Original=[96.39841672277781]\n",
      "Predicted=[188.71262], Original=[283.9337174365278]\n",
      "Predicted=[191.38657], Original=[178.70997284249998]\n",
      "Predicted=[193.3959], Original=[514.3822047222222]\n",
      "Predicted=[195.12967], Original=[584.90269]\n",
      "Predicted=[70.202065], Original=[0.030574016]\n",
      "Predicted=[105.80741], Original=[112.07683295662176]\n",
      "Predicted=[111.38473], Original=[35.789868289863264]\n",
      "Predicted=[108.97467], Original=[76.96528963905556]\n",
      "Predicted=[106.63953], Original=[555.3276455797222]\n",
      "Predicted=[105.15942], Original=[350.55066811685924]\n",
      "Predicted=[103.964], Original=[0.024405601069444446]\n",
      "Predicted=[103.08838], Original=[35.85716158624321]\n",
      "Predicted=[102.465034], Original=[36.87051946114123]\n",
      "Predicted=[101.971565], Original=[65.88829828722224]\n",
      "Predicted=[101.5418], Original=[37.95143452769912]\n",
      "Predicted=[101.14812], Original=[41.889351951527786]\n",
      "Predicted=[100.780235], Original=[584.4226766666661]\n",
      "Predicted=[100.43449], Original=[513.6088468055558]\n",
      "Predicted=[100.1097], Original=[35.67225290986111]\n",
      "Predicted=[99.805664], Original=[823.5428879166674]\n",
      "Predicted=[99.52252], Original=[152.96358011249995]\n",
      "Predicted=[99.26039], Original=[92.81046331888889]\n",
      "Predicted=[99.01928], Original=[510.43465277777784]\n",
      "Predicted=[98.79905], Original=[92.40986373818056]\n",
      "Predicted=[98.59938], Original=[83.67744258236112]\n",
      "Predicted=[98.4197], Original=[35.79171555682271]\n",
      "Predicted=[98.259285], Original=[243.68925669374994]\n",
      "Predicted=[98.11726], Original=[148.91568789041673]\n",
      "Predicted=[61.164967], Original=[70.37888652443574]\n",
      "Predicted=[91.86368], Original=[161.28777767930555]\n",
      "Predicted=[102.65876], Original=[174.14673291399293]\n",
      "Predicted=[106.382545], Original=[389.78119300277797]\n",
      "Predicted=[106.40353], Original=[104.96816865428814]\n",
      "Predicted=[103.99756], Original=[82.54472654305557]\n",
      "Predicted=[100.40043], Original=[511.07796277777766]\n",
      "Predicted=[96.403725], Original=[87.8111049883195]\n",
      "Predicted=[92.72002], Original=[589.2181184722217]\n",
      "Predicted=[89.930305], Original=[749.7476970833342]\n",
      "Predicted=[88.15883], Original=[42.91901455861111]\n",
      "Predicted=[87.208176], Original=[0.11477311999999999]\n",
      "Predicted=[86.78016], Original=[125.78124407263891]\n",
      "Predicted=[86.62275], Original=[42.36284019855501]\n",
      "Predicted=[86.58442], Original=[174.14673291399293]\n",
      "Predicted=[86.5915], Original=[43.85238027499998]\n",
      "Predicted=[86.61326], Original=[495.17819820222167]\n",
      "Predicted=[86.63882], Original=[182.14450404208313]\n",
      "Predicted=[86.66526], Original=[95.34981103763894]\n",
      "Predicted=[86.6923], Original=[149.07460795291664]\n",
      "Predicted=[86.720184], Original=[195.54036961263887]\n",
      "Predicted=[86.74912], Original=[174.14673291399293]\n",
      "Predicted=[86.7793], Original=[40.11326466081489]\n",
      "Predicted=[86.81063], Original=[95.51479954222219]\n",
      "Predicted=[80.02436], Original=[90.28044263736115]\n",
      "Predicted=[110.98676], Original=[138.09061380513882]\n",
      "Predicted=[109.94782], Original=[43.062970584305546]\n",
      "Predicted=[102.2137], Original=[518.1920811111114]\n",
      "Predicted=[96.989586], Original=[43.95955084081944]\n",
      "Predicted=[94.01432], Original=[35.79065997570302]\n",
      "Predicted=[92.88445], Original=[88.09970169381941]\n",
      "Predicted=[92.78327], Original=[102.25453682584477]\n",
      "Predicted=[92.932846], Original=[35.79382671906208]\n",
      "Predicted=[93.10551], Original=[77.11005018470831]\n",
      "Predicted=[93.2718], Original=[581.4114595833327]\n",
      "Predicted=[93.46018], Original=[64.04839102282334]\n",
      "Predicted=[93.66782], Original=[83.32783584443052]\n",
      "Predicted=[93.86172], Original=[514.9764515277784]\n",
      "Predicted=[94.03921], Original=[149.54124452319428]\n",
      "Predicted=[94.200615], Original=[784.9810672222222]\n",
      "Predicted=[94.346886], Original=[248.00524819402773]\n",
      "Predicted=[94.47919], Original=[515.2922626388887]\n",
      "Predicted=[94.59876], Original=[115.91604881983328]\n",
      "Predicted=[94.70788], Original=[172.67873177833343]\n",
      "Predicted=[94.808945], Original=[192.76222828291654]\n",
      "Predicted=[94.9009], Original=[35.82338299041328]\n",
      "Predicted=[94.97866], Original=[521.0765360312238]\n",
      "Predicted=[95.04742], Original=[36.87051946114123]\n",
      "Predicted=[70.245895], Original=[41.288218174000015]\n",
      "Predicted=[104.1386], Original=[504.2622726388893]\n",
      "Predicted=[120.49491], Original=[96.46445381180558]\n",
      "Predicted=[128.64609], Original=[150.710629793611]\n",
      "Predicted=[131.74228], Original=[66.43168123290279]\n",
      "Predicted=[132.2099], Original=[312.5038614334025]\n",
      "Predicted=[130.5628], Original=[0.0]\n",
      "Predicted=[127.10076], Original=[555.8238448611114]\n",
      "Predicted=[121.78188], Original=[37.95143452769912]\n",
      "Predicted=[114.867645], Original=[297.70723592666656]\n",
      "Predicted=[107.382195], Original=[94.98075177713885]\n",
      "Predicted=[101.11254], Original=[36.87051946114123]\n",
      "Predicted=[97.49316], Original=[76.52480360943058]\n",
      "Predicted=[95.835365], Original=[88.45212456111113]\n",
      "Predicted=[95.20672], Original=[193.93297942489588]\n",
      "Predicted=[94.99278], Original=[718.2941477777777]\n",
      "Predicted=[94.91847], Original=[42.29526858486112]\n",
      "Predicted=[94.88303], Original=[104.96816865428814]\n",
      "Predicted=[94.85888], Original=[35.80649369249831]\n",
      "Predicted=[94.841606], Original=[276.5662014238889]\n",
      "Predicted=[94.83109], Original=[199.31296103708326]\n",
      "Predicted=[94.827156], Original=[147.86483918902783]\n",
      "Predicted=[94.82925], Original=[172.54473946013886]\n",
      "Predicted=[94.83651], Original=[0.0710918325]\n",
      "Predicted=[65.90143], Original=[70.37888652443574]\n",
      "Predicted=[98.12647], Original=[91.17724964497224]\n",
      "Predicted=[113.29197], Original=[82.10747907013887]\n",
      "Predicted=[122.039665], Original=[35.92471877790308]\n",
      "Predicted=[125.07887], Original=[510.15146361111107]\n",
      "Predicted=[124.99403], Original=[88.80806665819439]\n",
      "Predicted=[122.2656], Original=[516.1245395000002]\n",
      "Predicted=[117.00306], Original=[162.68479291541672]\n",
      "Predicted=[110.669106], Original=[84.11665171513883]\n",
      "Predicted=[105.304115], Original=[571.2618488888894]\n",
      "Predicted=[101.85239], Original=[554.1805633333325]\n",
      "Predicted=[99.866585], Original=[165.86186050774745]\n",
      "Predicted=[98.73036], Original=[35.79065997570302]\n",
      "Predicted=[98.04389], Original=[44.43692492704645]\n",
      "Predicted=[97.59446], Original=[243.95686713930544]\n",
      "Predicted=[97.273186], Original=[338.0182540230557]\n",
      "Predicted=[97.025925], Original=[511.05549119444487]\n",
      "Predicted=[96.82584], Original=[512.8213458333333]\n",
      "Predicted=[96.65899], Original=[79.73202652588888]\n",
      "Predicted=[96.51743], Original=[207.59292206875008]\n",
      "Predicted=[96.396095], Original=[92.33960315111113]\n",
      "Predicted=[96.291504], Original=[54.141472935416665]\n",
      "Predicted=[96.20097], Original=[102.97012169305559]\n",
      "Predicted=[96.12235], Original=[76.93219841778755]\n",
      "Predicted=[66.22344], Original=[518.7482731944447]\n",
      "Predicted=[102.33464], Original=[501.6782365277779]\n",
      "Predicted=[124.744934], Original=[190.02782214250004]\n",
      "Predicted=[137.1543], Original=[0.09548352]\n",
      "Predicted=[139.89452], Original=[90.26121866916668]\n",
      "Predicted=[138.60782], Original=[70.37888652443574]\n",
      "Predicted=[137.22432], Original=[75.13733690916668]\n",
      "Predicted=[137.22766], Original=[589.2181184722217]\n",
      "Predicted=[138.44626], Original=[92.16704267805557]\n",
      "Predicted=[140.41154], Original=[83.41035592416665]\n",
      "Predicted=[142.98996], Original=[0.0]\n",
      "Predicted=[146.19391], Original=[96.77892122444446]\n",
      "Predicted=[149.99692], Original=[42.12811148986112]\n",
      "Predicted=[154.10965], Original=[35.78973634222331]\n",
      "Predicted=[158.00504], Original=[48.397318438472226]\n",
      "Predicted=[161.08018], Original=[36.87051946114123]\n",
      "Predicted=[163.01602], Original=[164.72426218102737]\n",
      "Predicted=[163.9631], Original=[124.95120771527777]\n",
      "Predicted=[164.37297], Original=[725.3069718055555]\n",
      "Predicted=[164.57062], Original=[174.03695843780733]\n",
      "Predicted=[164.6892], Original=[589.2181184722217]\n",
      "Predicted=[164.76996], Original=[157.11181508347212]\n",
      "Predicted=[164.82623], Original=[83.66249534513892]\n",
      "Predicted=[164.86476], Original=[317.10689927194477]\n",
      "Predicted=[65.55854], Original=[84.26625544583332]\n",
      "Predicted=[97.20603], Original=[90.94972796916664]\n",
      "Predicted=[109.132195], Original=[145.48382178749992]\n",
      "Predicted=[114.35982], Original=[344.1914777256907]\n",
      "Predicted=[117.266], Original=[97.4733148451389]\n",
      "Predicted=[117.42632], Original=[219.38869054708329]\n",
      "Predicted=[113.03715], Original=[881.6537002777787]\n",
      "Predicted=[104.240685], Original=[83.35729475055552]\n",
      "Predicted=[93.34227], Original=[35.78973634222331]\n",
      "Predicted=[86.328804], Original=[35.82338299041328]\n",
      "Predicted=[83.54502], Original=[93.02631658319443]\n",
      "Predicted=[82.6489], Original=[99.35889125625002]\n",
      "Predicted=[82.24767], Original=[96.23892863472223]\n",
      "Predicted=[81.95916], Original=[178.0567398063889]\n",
      "Predicted=[81.71302], Original=[78.60880995500001]\n",
      "Predicted=[81.50056], Original=[36.05983316122281]\n",
      "Predicted=[81.32032], Original=[89.20055038433331]\n",
      "Predicted=[81.168526], Original=[513.8790808333339]\n",
      "Predicted=[81.03986], Original=[55.75106428041667]\n",
      "Predicted=[80.92935], Original=[46.812129779597214]\n",
      "Predicted=[80.83292], Original=[37.95143452769912]\n",
      "Predicted=[80.74746], Original=[94.17934543805548]\n",
      "Predicted=[80.67068], Original=[0.2007583243032325]\n",
      "Predicted=[80.60083], Original=[35.82338299041328]\n",
      "Predicted=[101.72432], Original=[91.85685875333333]\n",
      "Predicted=[143.24988], Original=[42.26891059722224]\n",
      "Predicted=[155.33232], Original=[529.3511288888889]\n",
      "Predicted=[159.60934], Original=[592.3421954444448]\n",
      "Predicted=[160.13017], Original=[42.24952249638889]\n",
      "Predicted=[159.77791], Original=[589.2181184722217]\n",
      "Predicted=[158.91728], Original=[879.4608698611113]\n",
      "Predicted=[157.6531], Original=[589.2181184722217]\n",
      "Predicted=[156.29047], Original=[53.084245459509546]\n",
      "Predicted=[155.00409], Original=[53.084245459509546]\n",
      "Predicted=[153.88202], Original=[0.24063775999999998]\n",
      "Predicted=[152.93976], Original=[390.05004228985763]\n",
      "Predicted=[152.1612], Original=[738.5607937500002]\n",
      "Predicted=[151.5159], Original=[512.0033215277778]\n",
      "Predicted=[150.97293], Original=[99.61275580902776]\n",
      "Predicted=[150.50664], Original=[287.4802583333333]\n",
      "Predicted=[150.09753], Original=[43.55596978807003]\n",
      "Predicted=[149.73154], Original=[77.12530061486113]\n",
      "Predicted=[149.39867], Original=[881.5703555555564]\n",
      "Predicted=[149.09167], Original=[86.59288183861112]\n",
      "Predicted=[148.80534], Original=[217.50313241847203]\n",
      "Predicted=[148.5358], Original=[510.14440472222196]\n",
      "Predicted=[148.2796], Original=[0.0]\n",
      "Predicted=[148.0315], Original=[35.78967036840332]\n",
      "Predicted=[85.00013], Original=[37.95143452769912]\n",
      "Predicted=[119.29746], Original=[427.17248358461694]\n",
      "Predicted=[133.56772], Original=[35.85716158624321]\n",
      "Predicted=[136.88293], Original=[92.69587064166663]\n",
      "Predicted=[135.24844], Original=[372.3110776243053]\n",
      "Predicted=[135.14828], Original=[82.06375456194446]\n",
      "Predicted=[132.34114], Original=[194.9527143215738]\n",
      "Predicted=[128.42267], Original=[207.62203622541654]\n",
      "Predicted=[125.45547], Original=[515.9180145833329]\n",
      "Predicted=[123.88514], Original=[146.71660208541664]\n",
      "Predicted=[123.378975], Original=[35.92471877790308]\n",
      "Predicted=[123.68105], Original=[0.0675136]\n",
      "Predicted=[124.628716], Original=[530.1976626388889]\n",
      "Predicted=[126.120804], Original=[659.2504209722221]\n",
      "Predicted=[128.20697], Original=[832.0021654166666]\n",
      "Predicted=[131.02975], Original=[230.42831043454308]\n",
      "Predicted=[134.41388], Original=[97.11468643527776]\n",
      "Predicted=[138.13065], Original=[394.67722737569494]\n",
      "Predicted=[141.71362], Original=[37.95143452769912]\n",
      "Predicted=[144.76703], Original=[136.05920114499997]\n",
      "Predicted=[147.08823], Original=[280.82537201177377]\n",
      "Predicted=[148.6909], Original=[205.5544446499999]\n",
      "Predicted=[149.78082], Original=[513.7464584722222]\n",
      "Predicted=[150.51743], Original=[41.2183626475346]\n",
      "Predicted=[79.279144], Original=[91.30842933581948]\n",
      "Predicted=[102.59171], Original=[401.3791379218887]\n",
      "Predicted=[110.220665], Original=[81.13229357277781]\n",
      "Predicted=[117.480415], Original=[35.78967036840332]\n",
      "Predicted=[124.38829], Original=[43.946353368000004]\n",
      "Predicted=[130.82034], Original=[36.33006192786229]\n",
      "Predicted=[139.68317], Original=[180.60187989902775]\n",
      "Predicted=[148.39972], Original=[589.2181184722217]\n",
      "Predicted=[154.03989], Original=[89.40075852958327]\n",
      "Predicted=[156.014], Original=[512.4873393055558]\n",
      "Predicted=[156.44666], Original=[437.23617075638896]\n",
      "Predicted=[156.03596], Original=[884.4395130555563]\n",
      "Predicted=[155.01974], Original=[649.0012531944441]\n",
      "Predicted=[153.6456], Original=[515.7714349999994]\n",
      "Predicted=[152.12082], Original=[95.03799284416672]\n",
      "Predicted=[150.76662], Original=[882.984241111111]\n",
      "Predicted=[149.78271], Original=[613.5406340277777]\n",
      "Predicted=[149.22083], Original=[44.43692492704645]\n",
      "Predicted=[148.97125], Original=[157.10592999444418]\n",
      "Predicted=[148.91472], Original=[35.85716158624321]\n",
      "Predicted=[148.96289], Original=[43.69962755000001]\n",
      "Predicted=[149.06377], Original=[148.20080081847232]\n",
      "Predicted=[149.19032], Original=[66.39625014986109]\n",
      "Predicted=[149.32886], Original=[520.577950972223]\n",
      "Predicted=[93.497505], Original=[90.1070807013889]\n",
      "Predicted=[142.00772], Original=[43.194564884305535]\n",
      "Predicted=[155.1118], Original=[185.9749828058333]\n",
      "Predicted=[155.14815], Original=[509.213731527778]\n",
      "Predicted=[148.44739], Original=[104.96816865428814]\n",
      "Predicted=[139.98753], Original=[900.9706904166663]\n",
      "Predicted=[135.51166], Original=[35.79171555682271]\n",
      "Predicted=[133.48131], Original=[252.05823761277782]\n",
      "Predicted=[132.2345], Original=[40.11326466081489]\n",
      "Predicted=[131.23456], Original=[91.75652818694446]\n",
      "Predicted=[130.29721], Original=[510.2768797222223]\n",
      "Predicted=[129.3671], Original=[512.7242002777776]\n",
      "Predicted=[128.47076], Original=[45.17069256676386]\n",
      "Predicted=[127.62115], Original=[89.03297181986115]\n",
      "Predicted=[126.827324], Original=[79.11918453708334]\n",
      "Predicted=[126.09808], Original=[87.99671135970836]\n",
      "Predicted=[125.473976], Original=[36.05983316122281]\n",
      "Predicted=[124.89553], Original=[153.93904609274392]\n",
      "Predicted=[124.37455], Original=[37.95143452769912]\n",
      "Predicted=[123.908455], Original=[319.74004406680535]\n",
      "Predicted=[123.49737], Original=[98.75129490638888]\n",
      "Predicted=[123.131645], Original=[506.23471972222217]\n",
      "Predicted=[122.80701], Original=[96.78072142444445]\n",
      "Predicted=[122.51918], Original=[43.663217229444435]\n",
      "Predicted=[70.02047], Original=[0.0892144]\n",
      "Predicted=[100.961754], Original=[198.42055916861108]\n",
      "Predicted=[112.96338], Original=[528.7035825000006]\n",
      "Predicted=[120.758644], Original=[129.98198312861118]\n",
      "Predicted=[124.75357], Original=[88.09801764916664]\n",
      "Predicted=[127.10949], Original=[4.950866981944442]\n",
      "Predicted=[128.60858], Original=[35.79013218514318]\n",
      "Predicted=[129.54468], Original=[173.0048033686111]\n",
      "Predicted=[130.15475], Original=[641.2865672222222]\n",
      "Predicted=[130.57587], Original=[77.59379726005909]\n",
      "Predicted=[130.83008], Original=[106.32335651861113]\n",
      "Predicted=[130.87277], Original=[180.16201697069422]\n",
      "Predicted=[130.61002], Original=[564.2985691111106]\n",
      "Predicted=[129.90704], Original=[112.721858501333]\n",
      "Predicted=[128.61545], Original=[94.50440421194442]\n",
      "Predicted=[126.30974], Original=[519.6050373611118]\n",
      "Predicted=[122.8628], Original=[147.1331510247222]\n",
      "Predicted=[118.46208], Original=[727.6472219444438]\n",
      "Predicted=[113.68566], Original=[35.79171555682271]\n",
      "Predicted=[109.29624], Original=[90.33493882361117]\n",
      "Predicted=[106.05938], Original=[511.899134444444]\n",
      "Predicted=[103.95596], Original=[798.5421468055553]\n",
      "Predicted=[102.6531], Original=[0.3472128]\n",
      "Predicted=[101.83366], Original=[150.59454511597215]\n",
      "Predicted=[76.288246], Original=[508.4770286111108]\n",
      "Predicted=[114.50927], Original=[85.3943673211111]\n",
      "Predicted=[135.40071], Original=[85.9340742070695]\n",
      "Predicted=[142.68637], Original=[40.11326466081489]\n",
      "Predicted=[138.24878], Original=[43.535885591111146]\n",
      "Predicted=[133.17812], Original=[148.51114563576]\n",
      "Predicted=[130.02016], Original=[103.42951221013885]\n",
      "Predicted=[127.36277], Original=[43.59470581777777]\n",
      "Predicted=[125.05857], Original=[40.11326466081489]\n",
      "Predicted=[123.19488], Original=[35.789868289863264]\n",
      "Predicted=[121.80669], Original=[36.33006192786229]\n",
      "Predicted=[120.86496], Original=[881.8394594444441]\n",
      "Predicted=[120.27241], Original=[229.4237991934721]\n",
      "Predicted=[119.92014], Original=[300.24228421914495]\n",
      "Predicted=[119.72253], Original=[53.084245459509546]\n",
      "Predicted=[119.62068], Original=[157.03882952144187]\n",
      "Predicted=[119.57691], Original=[82.08754003180559]\n",
      "Predicted=[119.56772], Original=[292.977216011111]\n",
      "Predicted=[119.57892], Original=[84.99902208208337]\n",
      "Predicted=[119.60186], Original=[94.96400503306947]\n",
      "Predicted=[119.63142], Original=[509.8073376111114]\n",
      "Predicted=[119.664505], Original=[258.1655387397221]\n",
      "Predicted=[119.69923], Original=[68.6748139376389]\n",
      "Predicted=[119.73447], Original=[42.61662566416673]\n",
      "Predicted=[64.675995], Original=[917.0841406944436]\n",
      "Predicted=[101.41845], Original=[216.00561259694445]\n",
      "Predicted=[113.08283], Original=[53.42217429444444]\n",
      "Predicted=[118.4248], Original=[516.8472073611111]\n",
      "Predicted=[120.89184], Original=[486.6028501388888]\n",
      "Predicted=[121.590126], Original=[510.44853125000003]\n",
      "Predicted=[121.947914], Original=[83.36565152576387]\n",
      "Predicted=[122.20488], Original=[36.05983316122281]\n",
      "Predicted=[122.36857], Original=[35.80649369249831]\n",
      "Predicted=[122.44477], Original=[99.39168347083337]\n",
      "Predicted=[122.438866], Original=[90.18466318250003]\n",
      "Predicted=[122.34939], Original=[515.926015694444]\n",
      "Predicted=[122.16709], Original=[44.43692492704645]\n",
      "Predicted=[121.87043], Original=[35.61260089083333]\n",
      "Predicted=[121.465096], Original=[45.5926678623611]\n",
      "Predicted=[120.96209], Original=[891.8706005555559]\n",
      "Predicted=[120.36264], Original=[126.0867228257688]\n",
      "Predicted=[119.675446], Original=[95.98643384333336]\n",
      "Predicted=[118.90191], Original=[720.5219268055549]\n",
      "Predicted=[118.044044], Original=[76.96131475000001]\n",
      "Predicted=[117.1043], Original=[35.79804904354083]\n",
      "Predicted=[116.08402], Original=[36.87051946114123]\n",
      "Predicted=[114.99612], Original=[41.09210675680555]\n",
      "Predicted=[113.85566], Original=[6.746743415680556]\n",
      "Predicted=[91.850716], Original=[0.39655122555555555]\n",
      "Predicted=[107.04027], Original=[45.59906551069445]\n",
      "Predicted=[109.864845], Original=[81.60037451429169]\n",
      "Predicted=[110.91514], Original=[35.78973634222331]\n",
      "Predicted=[111.9979], Original=[327.9349360791662]\n",
      "Predicted=[112.85189], Original=[603.4576293055565]\n",
      "Predicted=[113.608955], Original=[41.97783186808333]\n",
      "Predicted=[114.34937], Original=[161.87981042708336]\n",
      "Predicted=[115.0735], Original=[36.87051946114123]\n",
      "Predicted=[115.784676], Original=[511.45723402777776]\n",
      "Predicted=[116.48528], Original=[44.43692492704645]\n",
      "Predicted=[117.19262], Original=[641.9844412500006]\n",
      "Predicted=[117.916176], Original=[509.7230488888889]\n",
      "Predicted=[118.65011], Original=[104.36620558763885]\n",
      "Predicted=[119.39014], Original=[341.12941100291675]\n",
      "Predicted=[120.13514], Original=[511.96587583333366]\n",
      "Predicted=[120.88545], Original=[154.07433688748603]\n",
      "Predicted=[121.641045], Original=[82.95611232333336]\n",
      "Predicted=[122.401566], Original=[40.11326466081489]\n",
      "Predicted=[123.16645], Original=[146.06599060958334]\n",
      "Predicted=[123.93513], Original=[40.11326466081489]\n",
      "Predicted=[124.70705], Original=[83.1824019192361]\n",
      "Predicted=[125.481575], Original=[201.60922149835582]\n",
      "Predicted=[126.25823], Original=[860.2599690277766]\n",
      "Predicted=[88.14429], Original=[570.4453501388892]\n",
      "Predicted=[135.8419], Original=[908.3759630555555]\n",
      "Predicted=[154.77707], Original=[42.96279220819444]\n",
      "Predicted=[158.04298], Original=[42.485507544305555]\n",
      "Predicted=[156.39766], Original=[56.66311136693054]\n",
      "Predicted=[150.07393], Original=[35.789868289863264]\n",
      "Predicted=[146.49225], Original=[508.9250327777775]\n",
      "Predicted=[144.85123], Original=[101.31126516611108]\n",
      "Predicted=[144.10413], Original=[127.99843079666671]\n",
      "Predicted=[143.68388], Original=[103.66063153652786]\n",
      "Predicted=[143.46771], Original=[174.14673291399293]\n",
      "Predicted=[143.36075], Original=[104.96816865428814]\n",
      "Predicted=[143.32391], Original=[0.02796992]\n",
      "Predicted=[143.33113], Original=[36.33006192786229]\n",
      "Predicted=[143.36678], Original=[312.5038614334025]\n",
      "Predicted=[143.42023], Original=[154.29471448902785]\n",
      "Predicted=[143.48494], Original=[57.514420480277785]\n",
      "Predicted=[143.55664], Original=[497.37263944444413]\n",
      "Predicted=[143.63269], Original=[35.85716158624321]\n",
      "Predicted=[143.71135], Original=[512.4283552777782]\n",
      "Predicted=[143.79153], Original=[0.0217008]\n",
      "Predicted=[143.8725], Original=[573.6646241666666]\n",
      "Predicted=[143.9538], Original=[35.92471877790308]\n",
      "Predicted=[144.03516], Original=[91.31507781097217]\n",
      "Predicted=[61.221306], Original=[343.0792770967469]\n",
      "Predicted=[69.21296], Original=[35.79013218514318]\n",
      "Predicted=[70.66563], Original=[44.54717027833334]\n",
      "Predicted=[70.87239], Original=[84.4228075291666]\n",
      "Predicted=[71.28154], Original=[151.68352450916663]\n",
      "Predicted=[71.55663], Original=[35.82338299041328]\n",
      "Predicted=[71.799034], Original=[35.79171555682271]\n",
      "Predicted=[72.00616], Original=[244.41406041068072]\n",
      "Predicted=[72.20254], Original=[509.6771936111102]\n",
      "Predicted=[72.39073], Original=[45.947399023362884]\n",
      "Predicted=[72.57288], Original=[61.03755316847219]\n",
      "Predicted=[72.75013], Original=[225.62639676850665]\n",
      "Predicted=[72.92329], Original=[514.3900124999993]\n",
      "Predicted=[73.092896], Original=[44.43692492704645]\n",
      "Predicted=[73.259384], Original=[97.1864416705267]\n",
      "Predicted=[73.42302], Original=[42.1964404227778]\n",
      "Predicted=[73.58405], Original=[609.3389531944442]\n",
      "Predicted=[73.74268], Original=[0.07522944]\n",
      "Predicted=[73.89902], Original=[43.809798752933396]\n",
      "Predicted=[74.05318], Original=[82.89806150055556]\n",
      "Predicted=[74.20528], Original=[377.002004472123]\n",
      "Predicted=[74.35538], Original=[35.78973634222331]\n",
      "Predicted=[74.51038], Original=[97.08671150333336]\n",
      "Predicted=[74.666504], Original=[511.83036152777777]\n",
      "Predicted=[68.53983], Original=[392.99463205884723]\n",
      "Predicted=[86.54371], Original=[515.7855554166666]\n",
      "Predicted=[86.693344], Original=[172.9887806876388]\n",
      "Predicted=[87.809006], Original=[35.79382671906208]\n",
      "Predicted=[87.71714], Original=[132.64570319606943]\n",
      "Predicted=[87.81118], Original=[104.96816865428814]\n",
      "Predicted=[88.16819], Original=[276.8314484318057]\n",
      "Predicted=[88.72219], Original=[92.98370268708334]\n",
      "Predicted=[89.42736], Original=[539.8580250000001]\n",
      "Predicted=[90.247536], Original=[35.78967036840332]\n",
      "Predicted=[91.16372], Original=[43.095047923749995]\n",
      "Predicted=[92.157394], Original=[35.79804904354083]\n",
      "Predicted=[93.19361], Original=[173.29450481069446]\n",
      "Predicted=[94.2371], Original=[44.52755741843916]\n",
      "Predicted=[95.25713], Original=[36.33006192786229]\n",
      "Predicted=[96.21511], Original=[35.79171555682271]\n",
      "Predicted=[97.0841], Original=[514.6502906111107]\n",
      "Predicted=[97.83761], Original=[189.3268249444445]\n",
      "Predicted=[98.45587], Original=[88.65097286847218]\n",
      "Predicted=[98.928505], Original=[95.7615750341667]\n",
      "Predicted=[99.255516], Original=[213.84334691430564]\n",
      "Predicted=[99.44639], Original=[94.47960762669477]\n",
      "Predicted=[99.51766], Original=[511.30754313888866]\n",
      "Predicted=[99.489586], Original=[0.00434016]\n",
      "Predicted=[64.04596], Original=[163.0485722070832]\n",
      "Predicted=[90.89777], Original=[81.32154629083333]\n",
      "Predicted=[102.05449], Original=[204.96970723636827]\n",
      "Predicted=[102.73666], Original=[90.8723571146083]\n",
      "Predicted=[99.430275], Original=[102.0885282311389]\n",
      "Predicted=[97.40076], Original=[36.05983316122281]\n",
      "Predicted=[96.86912], Original=[488.3167886111107]\n",
      "Predicted=[97.08918], Original=[170.56063827693046]\n",
      "Predicted=[97.41046], Original=[43.028077531388895]\n",
      "Predicted=[97.63467], Original=[0.11091519999999999]\n",
      "Predicted=[97.76068], Original=[91.05541183958341]\n",
      "Predicted=[97.82556], Original=[570.9148974999998]\n",
      "Predicted=[97.85969], Original=[38.099972527500015]\n",
      "Predicted=[97.881], Original=[0.14129632]\n",
      "Predicted=[97.89843], Original=[35.79171555682271]\n",
      "Predicted=[97.915764], Original=[432.3410382645835]\n",
      "Predicted=[97.93429], Original=[35.82338299041328]\n",
      "Predicted=[97.95423], Original=[515.582383888889]\n",
      "Predicted=[97.97544], Original=[35.85716158624321]\n",
      "Predicted=[97.997734], Original=[77.4118270897222]\n",
      "Predicted=[98.02096], Original=[508.9862986111107]\n",
      "Predicted=[98.04502], Original=[511.0460972222223]\n",
      "Predicted=[98.06994], Original=[518.3223625000003]\n",
      "Predicted=[98.09568], Original=[174.14673291399293]\n",
      "Predicted=[55.79924], Original=[513.7993725]\n",
      "Predicted=[69.21767], Original=[90.86583121694447]\n",
      "Predicted=[72.6593], Original=[0.0]\n",
      "Predicted=[74.14744], Original=[571.8354304166666]\n",
      "Predicted=[75.526375], Original=[35.85716158624321]\n",
      "Predicted=[76.751076], Original=[578.284527083334]\n",
      "Predicted=[77.906136], Original=[41.98682091291667]\n",
      "Predicted=[79.03354], Original=[35.85716158624321]\n",
      "Predicted=[80.13897], Original=[0.31422075877319433]\n",
      "Predicted=[81.209595], Original=[35.78967036840332]\n",
      "Predicted=[82.136986], Original=[43.910767669027805]\n",
      "Predicted=[82.87881], Original=[528.0295416666673]\n",
      "Predicted=[83.44042], Original=[558.1304299999999]\n",
      "Predicted=[83.784035], Original=[35.78967036840332]\n",
      "Predicted=[83.89745], Original=[499.25052222222223]\n",
      "Predicted=[83.82156], Original=[117.53920444833332]\n",
      "Predicted=[83.58592], Original=[499.3906794444445]\n",
      "Predicted=[83.28019], Original=[43.0412511911111]\n",
      "Predicted=[83.10595], Original=[44.07477899637499]\n",
      "Predicted=[83.15884], Original=[225.58206111861094]\n",
      "Predicted=[83.43092], Original=[96.47121405138891]\n",
      "Predicted=[83.88697], Original=[510.39833236111144]\n",
      "Predicted=[84.4654], Original=[35.92471877790308]\n",
      "Predicted=[85.115685], Original=[341.2687461000379]\n",
      "Predicted=[66.43626], Original=[35.92471877790308]\n",
      "Predicted=[78.88878], Original=[46.81404456922135]\n",
      "Predicted=[80.69247], Original=[511.04420458333306]\n",
      "Predicted=[82.83273], Original=[95.67588946361113]\n",
      "Predicted=[84.983635], Original=[565.3037029166667]\n",
      "Predicted=[86.95989], Original=[516.4658011111107]\n",
      "Predicted=[88.85004], Original=[509.2309436111111]\n",
      "Predicted=[90.582115], Original=[458.46199377108354]\n",
      "Predicted=[92.12533], Original=[40.11326466081489]\n",
      "Predicted=[93.527336], Original=[487.37016690611097]\n",
      "Predicted=[94.81866], Original=[465.4205249525336]\n",
      "Predicted=[96.00489], Original=[19.12645740884722]\n",
      "Predicted=[97.0819], Original=[163.3501073891665]\n",
      "Predicted=[98.14447], Original=[174.14673291399293]\n",
      "Predicted=[99.21894], Original=[44.41627500944443]\n",
      "Predicted=[100.31448], Original=[90.18920949276387]\n",
      "Predicted=[101.43344], Original=[43.87017809847222]\n",
      "Predicted=[102.57422], Original=[35.789868289863264]\n",
      "Predicted=[103.74436], Original=[286.460201657595]\n",
      "Predicted=[104.94723], Original=[509.8239893055553]\n",
      "Predicted=[106.1775], Original=[496.0953843055558]\n",
      "Predicted=[107.42996], Original=[76.9692645281111]\n",
      "Predicted=[108.6995], Original=[42.57431763290277]\n",
      "Predicted=[109.98232], Original=[522.7381411111108]\n"
     ]
    }
   ],
   "source": [
    "# show the inputs, predicted outputs and real outputs\n",
    "for i in range(len(X_test[0])):\n",
    "        for j in range(len(X_test[1])):\n",
    "            print(\"Predicted=%s, Original=%s\" % (y_predicted[i][j], y_test[i][j]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3144, 1)\n",
      "(3144, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with a prediction for all datapoints for the csv\n",
    "y_predicted_all = model.predict(X_shaped)\n",
    "y_pred_all = np.reshape(y_predicted_all, (-1, 1))\n",
    "print(y_pred_all.shape)\n",
    "y_pred_all = pd.DataFrame(data=y_pred_all)\n",
    "print(TimeHour.shape)\n",
    "my_df = pd.DataFrame({'Time': TimeHour.loc[:,'TimeHour'].tolist(), 'pred_y': y_pred_all.loc[:,0].tolist()})\n",
    "my_df.to_csv('WetPredictions.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def fill_with_average(df, previous, following):\n",
    "    value_new = list(df.loc[previous])\n",
    "    value_previous = list(df.loc[previous])\n",
    "    value_following = list(df.loc[following])\n",
    "    value_new[0]= (value_previous[0]+value_following[0])/2\n",
    "    date = df.loc[previous, 'TimeHour2']\n",
    "    new_date = date + timedelta(hours=1)\n",
    "    value_new[40]=new_date\n",
    "    value_new[date.hour]=0.0\n",
    "    value_new[new_date.hour]=1.0\n",
    "    #value_new[43]=1.0\n",
    "    df_length = len(df)\n",
    "    df.loc[df_length] = value_new\n",
    "    df = df.sort_values(by= \"TimeHour2\")\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=[\"index\"])\n",
    "    return df\n",
    "\n",
    "def Y_fill_with_average(df, previous, following):\n",
    "    value_new = list(df.loc[previous])\n",
    "    value_previous = list(df.loc[previous])\n",
    "    value_following = list(df.loc[following])\n",
    "    value_new[0]= (value_previous[0]+value_following[0])/2\n",
    "    date = df.loc[previous, 'TimeHour2']\n",
    "    new_date = date + timedelta(hours=1)\n",
    "    value_new[2]=new_date\n",
    "    #value_new[date.hour]=0.0\n",
    "    #value_new[new_date.hour]=1.0\n",
    "    #value_new[43]=1.0\n",
    "    df_length = len(df)\n",
    "    df.loc[df_length] = value_new\n",
    "    df = df.sort_values(by= \"TimeHour2\")\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=[\"index\"])\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "                 0             TimeHour           TimeHour2              Diff  \\\n550      47.628271  2018-01-25 14:00:00 2018-01-25 14:00:00   1 days 04:00:00   \n662       0.000000  2018-02-05 16:00:00 2018-02-05 16:00:00   4 days 05:00:00   \n1037     76.974656  2018-02-23 09:00:00 2018-02-23 09:00:00   2 days 03:00:00   \n1110     91.311921  2018-02-28 13:00:00 2018-02-28 13:00:00   0 days 04:00:00   \n1479     35.789604  2018-04-05 20:00:00 2018-04-05 20:00:00  20 days 23:00:00   \n1661      0.000000  2018-04-13 16:00:00 2018-04-13 16:00:00   0 days 06:00:00   \n1774     45.289669  2018-04-18 17:00:00 2018-04-18 17:00:00   0 days 09:00:00   \n1793     45.334073  2018-04-20 12:00:00 2018-04-20 12:00:00   1 days 01:00:00   \n2133     46.871539  2018-05-04 18:00:00 2018-05-04 18:00:00   0 days 03:00:00   \n3747     43.357102  2018-07-11 01:00:00 2018-07-11 01:00:00   0 days 02:00:00   \n4806      0.909036  2018-08-24 06:00:00 2018-08-24 06:00:00   0 days 03:00:00   \n5268     47.419004  2018-09-12 15:00:00 2018-09-12 15:00:00   0 days 04:00:00   \n6622   4587.106287  2018-11-08 08:00:00 2018-11-08 08:00:00   0 days 08:00:00   \n10024    43.196841  2019-03-31 03:00:00 2019-03-31 03:00:00   0 days 02:00:00   \n12649    76.969265  2019-07-18 13:00:00 2019-07-18 13:00:00   0 days 02:00:00   \n\n                    Before  diff_hours  \n550    2018-01-24 10:00:00        28.0  \n662    2018-02-01 11:00:00       101.0  \n1037   2018-02-21 06:00:00        51.0  \n1110   2018-02-28 09:00:00         4.0  \n1479   2018-03-15 21:00:00       503.0  \n1661   2018-04-13 10:00:00         6.0  \n1774   2018-04-18 08:00:00         9.0  \n1793   2018-04-19 11:00:00        25.0  \n2133   2018-05-04 15:00:00         3.0  \n3747   2018-07-10 23:00:00         2.0  \n4806   2018-08-24 03:00:00         3.0  \n5268   2018-09-12 11:00:00         4.0  \n6622   2018-11-08 00:00:00         8.0  \n10024  2019-03-31 01:00:00         2.0  \n12649  2019-07-18 11:00:00         2.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>TimeHour</th>\n      <th>TimeHour2</th>\n      <th>Diff</th>\n      <th>Before</th>\n      <th>diff_hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>550</th>\n      <td>47.628271</td>\n      <td>2018-01-25 14:00:00</td>\n      <td>2018-01-25 14:00:00</td>\n      <td>1 days 04:00:00</td>\n      <td>2018-01-24 10:00:00</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>662</th>\n      <td>0.000000</td>\n      <td>2018-02-05 16:00:00</td>\n      <td>2018-02-05 16:00:00</td>\n      <td>4 days 05:00:00</td>\n      <td>2018-02-01 11:00:00</td>\n      <td>101.0</td>\n    </tr>\n    <tr>\n      <th>1037</th>\n      <td>76.974656</td>\n      <td>2018-02-23 09:00:00</td>\n      <td>2018-02-23 09:00:00</td>\n      <td>2 days 03:00:00</td>\n      <td>2018-02-21 06:00:00</td>\n      <td>51.0</td>\n    </tr>\n    <tr>\n      <th>1110</th>\n      <td>91.311921</td>\n      <td>2018-02-28 13:00:00</td>\n      <td>2018-02-28 13:00:00</td>\n      <td>0 days 04:00:00</td>\n      <td>2018-02-28 09:00:00</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>35.789604</td>\n      <td>2018-04-05 20:00:00</td>\n      <td>2018-04-05 20:00:00</td>\n      <td>20 days 23:00:00</td>\n      <td>2018-03-15 21:00:00</td>\n      <td>503.0</td>\n    </tr>\n    <tr>\n      <th>1661</th>\n      <td>0.000000</td>\n      <td>2018-04-13 16:00:00</td>\n      <td>2018-04-13 16:00:00</td>\n      <td>0 days 06:00:00</td>\n      <td>2018-04-13 10:00:00</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>1774</th>\n      <td>45.289669</td>\n      <td>2018-04-18 17:00:00</td>\n      <td>2018-04-18 17:00:00</td>\n      <td>0 days 09:00:00</td>\n      <td>2018-04-18 08:00:00</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>1793</th>\n      <td>45.334073</td>\n      <td>2018-04-20 12:00:00</td>\n      <td>2018-04-20 12:00:00</td>\n      <td>1 days 01:00:00</td>\n      <td>2018-04-19 11:00:00</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>2133</th>\n      <td>46.871539</td>\n      <td>2018-05-04 18:00:00</td>\n      <td>2018-05-04 18:00:00</td>\n      <td>0 days 03:00:00</td>\n      <td>2018-05-04 15:00:00</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3747</th>\n      <td>43.357102</td>\n      <td>2018-07-11 01:00:00</td>\n      <td>2018-07-11 01:00:00</td>\n      <td>0 days 02:00:00</td>\n      <td>2018-07-10 23:00:00</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4806</th>\n      <td>0.909036</td>\n      <td>2018-08-24 06:00:00</td>\n      <td>2018-08-24 06:00:00</td>\n      <td>0 days 03:00:00</td>\n      <td>2018-08-24 03:00:00</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>5268</th>\n      <td>47.419004</td>\n      <td>2018-09-12 15:00:00</td>\n      <td>2018-09-12 15:00:00</td>\n      <td>0 days 04:00:00</td>\n      <td>2018-09-12 11:00:00</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>6622</th>\n      <td>4587.106287</td>\n      <td>2018-11-08 08:00:00</td>\n      <td>2018-11-08 08:00:00</td>\n      <td>0 days 08:00:00</td>\n      <td>2018-11-08 00:00:00</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>10024</th>\n      <td>43.196841</td>\n      <td>2019-03-31 03:00:00</td>\n      <td>2019-03-31 03:00:00</td>\n      <td>0 days 02:00:00</td>\n      <td>2019-03-31 01:00:00</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>12649</th>\n      <td>76.969265</td>\n      <td>2019-07-18 13:00:00</td>\n      <td>2019-07-18 13:00:00</td>\n      <td>0 days 02:00:00</td>\n      <td>2019-07-18 11:00:00</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(data=X)\n",
    "df_test['TimeHour'] = TimeHour.loc[:, 'TimeHour']\n",
    "df_test['TimeHour2'] = 'NA'\n",
    "df_test['TimeHour2'] = pd.to_datetime(df_test['TimeHour'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_test['Diff'] = 'NA'\n",
    "df_test['Before']=\"NA\"\n",
    "for x in range(1, df_test.shape[0]):\n",
    "    before = df_test.loc[x - 1, 'TimeHour2']\n",
    "    df_test.loc[x, 'Before'] = before\n",
    "    now = df_test.loc[x, 'TimeHour2']\n",
    "    diff = now - before\n",
    "    df_test.loc[x, 'Diff'] = diff\n",
    "df_test\n",
    "df_test = df_test.drop(0)\n",
    "df_test['diff_hours'] = [x.total_seconds() / 3600 for x in df_test['Diff']]\n",
    "\n",
    "\n",
    "\n",
    "df_test = df_test.drop([1141,1142])\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns=[\"index\"])\n",
    "df_test = df_test.drop([655,656,657,658,659,660,661,662,663,664,665,666,667,668,669])\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns=[\"index\"])\n",
    "df_test = df_test.drop([641,642,643,644,645,646,647,648,649,650,651,652,653,654,655])\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns=[\"index\"])\n",
    "df_test = df_test.drop([1480])\n",
    "df_test = df_test.drop([7900,7901,7902,7903,7904,7905])\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns=[\"index\"])\n",
    "df_test[df_test['diff_hours']>1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1844\n",
      "2185\n",
      "2186\n",
      "3801\n",
      "4861\n",
      "4862\n",
      "5325\n",
      "5326\n",
      "5327\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6686\n",
      "6687\n",
      "6688\n",
      "10091\n",
      "12717\n",
      "Missing values approximated\n"
     ]
    },
    {
     "data": {
      "text/plain": "              0             TimeHour           TimeHour2              Diff  \\\n554   47.628271  2018-01-25 14:00:00 2018-01-25 14:00:00   1 days 04:00:00   \n671    0.000000  2018-02-05 16:00:00 2018-02-05 16:00:00   4 days 05:00:00   \n1049  76.974656  2018-02-23 09:00:00 2018-02-23 09:00:00   2 days 03:00:00   \n1517  35.789604  2018-04-05 20:00:00 2018-04-05 20:00:00  20 days 23:00:00   \n1845  45.334073  2018-04-20 12:00:00 2018-04-20 12:00:00   1 days 01:00:00   \n\n                   Before  diff_hours  \n554   2018-01-24 10:00:00        24.0  \n671   2018-02-01 11:00:00        96.0  \n1049  2018-02-21 06:00:00        48.0  \n1517  2018-03-15 21:00:00       480.0  \n1845  2018-04-19 11:00:00        24.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>TimeHour</th>\n      <th>TimeHour2</th>\n      <th>Diff</th>\n      <th>Before</th>\n      <th>diff_hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>554</th>\n      <td>47.628271</td>\n      <td>2018-01-25 14:00:00</td>\n      <td>2018-01-25 14:00:00</td>\n      <td>1 days 04:00:00</td>\n      <td>2018-01-24 10:00:00</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>671</th>\n      <td>0.000000</td>\n      <td>2018-02-05 16:00:00</td>\n      <td>2018-02-05 16:00:00</td>\n      <td>4 days 05:00:00</td>\n      <td>2018-02-01 11:00:00</td>\n      <td>96.0</td>\n    </tr>\n    <tr>\n      <th>1049</th>\n      <td>76.974656</td>\n      <td>2018-02-23 09:00:00</td>\n      <td>2018-02-23 09:00:00</td>\n      <td>2 days 03:00:00</td>\n      <td>2018-02-21 06:00:00</td>\n      <td>48.0</td>\n    </tr>\n    <tr>\n      <th>1517</th>\n      <td>35.789604</td>\n      <td>2018-04-05 20:00:00</td>\n      <td>2018-04-05 20:00:00</td>\n      <td>20 days 23:00:00</td>\n      <td>2018-03-15 21:00:00</td>\n      <td>480.0</td>\n    </tr>\n    <tr>\n      <th>1845</th>\n      <td>45.334073</td>\n      <td>2018-04-20 12:00:00</td>\n      <td>2018-04-20 12:00:00</td>\n      <td>1 days 01:00:00</td>\n      <td>2018-04-19 11:00:00</td>\n      <td>24.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0\n",
    "while x < 13000:\n",
    "    #if df_test.loc[x, 'diff_hours'] != 1.0 and df_test.loc[x, \"diff_hours\"] != 24.0 and df_test.loc[x, 'diff_hours'] != 96.0 and df_test.loc[x, 'diff_hours'] != 48.0 and df_test.loc[x, 'diff_hours'] != 480.0:\n",
    "    if df_test.loc[x,'diff_hours'] %24!=0.0 and df_test.loc[x,'diff_hours'] != 1.0:\n",
    "        print(x)\n",
    "        df_test.loc[x, 'diff_hours'] = df_test.loc[x, 'diff_hours'] - 1\n",
    "        df_test = Y_fill_with_average(df_test, x-1, x)\n",
    "        x = x - 1\n",
    "    else:\n",
    "        x = x + 1\n",
    "\n",
    "print ('Missing values approximated')\n",
    "df_test[df_test['diff_hours']>1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13632, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                0             TimeHour\n0      502.995390  2018-01-01 13:00:00\n1      501.561763  2018-01-01 14:00:00\n2      267.722037  2018-01-01 15:00:00\n3      164.684623  2018-01-01 16:00:00\n4      415.606068  2018-01-01 17:00:00\n...           ...                  ...\n13627   95.319013  2019-08-25 10:00:00\n13628  158.127773  2019-08-25 11:00:00\n13629  107.449515  2019-08-25 12:00:00\n13630  184.320927  2019-08-25 13:00:00\n13631   84.346758  2019-08-25 14:00:00\n\n[13632 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>TimeHour</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>502.995390</td>\n      <td>2018-01-01 13:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>501.561763</td>\n      <td>2018-01-01 14:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>267.722037</td>\n      <td>2018-01-01 15:00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>164.684623</td>\n      <td>2018-01-01 16:00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>415.606068</td>\n      <td>2018-01-01 17:00:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13627</th>\n      <td>95.319013</td>\n      <td>2019-08-25 10:00:00</td>\n    </tr>\n    <tr>\n      <th>13628</th>\n      <td>158.127773</td>\n      <td>2019-08-25 11:00:00</td>\n    </tr>\n    <tr>\n      <th>13629</th>\n      <td>107.449515</td>\n      <td>2019-08-25 12:00:00</td>\n    </tr>\n    <tr>\n      <th>13630</th>\n      <td>184.320927</td>\n      <td>2019-08-25 13:00:00</td>\n    </tr>\n    <tr>\n      <th>13631</th>\n      <td>84.346758</td>\n      <td>2019-08-25 14:00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>13632 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test = df_test.drop(['Diff', 'TimeHour2', 'diff_hours', 'Before'],axis=1)\n",
    "df_test = df_test.drop([13632,13633,13634,13635,13636,13637,13638,13639,13640])\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "np.save('Y_cleaned.npy', df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "LZS_run_code_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}