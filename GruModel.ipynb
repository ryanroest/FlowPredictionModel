{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# for the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, RepeatVector, TimeDistributed, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[554.1805633333325, 0.0, 0.0, ..., 0.0, 1.0, 1.0],\n       [512.610779583333, 1.0, 0.0, ..., 0.0, 0.0, 1.0],\n       [495.14290208333324, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n       ...,\n       [113.38789088512496, 0.0, 0.0, ..., 0.0, 0.0, 1.0],\n       [77.43711137416668, 0.0, 0.0, ..., 0.0, 0.0, 1.0],\n       [163.3501073891665, 0.0, 0.0, ..., 0.0, 0.0, 1.0]], dtype=object)"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load files from pre-processing\n",
    "\n",
    "#X = np.load('X.npy') #rainpredictions, hour of day, month of year, holiday, ones\n",
    "#y = np.load('y.npy')\n",
    "\n",
    "X_dry = np.load('X_dry.npy',allow_pickle=True)\n",
    "y_dry = np.load('Y_dry.npy',allow_pickle=True)\n",
    "X_rain = np.load('X_rain.npy',allow_pickle=True)\n",
    "y_rain = np.load('Y_rain.npy',allow_pickle=True)\n",
    "TimeHour = pd.read_csv('Timehour_wet.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# split dataset in test and train\n",
    "\n",
    "def ReshapeXData(dataframe):\n",
    "    n_hours = 24\n",
    "    n_features = dataframe.shape[1]\n",
    "    dataframe_shaped = np.reshape(dataframe,(-1, n_hours, n_features))\n",
    "    return dataframe_shaped\n",
    "\n",
    "def ReshapeYData(dataframe):\n",
    "    n_hours = 24\n",
    "    n_features = 1\n",
    "    dataframe_shaped = np.reshape(dataframe,(-1, n_hours, n_features))\n",
    "    return dataframe_shaped\n",
    "\n",
    "#test_size for X_rain = 0.1984 as to properly split the set into groups of 24\n",
    "#test_size for X_dry = 0.2 for a proper split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rain, y_rain, test_size=0.1984, random_state=42)\n",
    "\n",
    "X_shaped = ReshapeXData(X_rain)\n",
    "X_train = ReshapeXData(X_train)\n",
    "X_test = ReshapeXData(X_test)\n",
    "y_train = ReshapeYData(y_train)\n",
    "y_test = ReshapeYData(y_test)\n",
    "\n",
    "#X_train_gru = X[0:10800,:]\n",
    "#X_test_gru = X[10801:13609,:]\n",
    "#Y_train_gru = y[0:10800]\n",
    "#Y_test_gru = y[10801:13609]\n",
    "\n",
    "\n",
    "#X_train_gru_shaped=ReshapeXData(X_train_gru)\n",
    "#X_test_gru_shaped = ReshapeXData(X_test_gru)\n",
    "#Y_train_gru_shaped = ReshapeYData(Y_train_gru)\n",
    "#Y_test_gru_shaped = ReshapeYData(Y_test_gru)\n",
    "#X_shaped = ReshapeXData(X[0:13608,:])\n",
    "#Y_shaped = ReshapeYData(y[0:13608])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 84 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 215.6972 - val_loss: 211.8116\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 206.7284 - val_loss: 202.5882\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 197.7885 - val_loss: 194.9771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 189.3910 - val_loss: 187.3199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 181.1372 - val_loss: 180.5817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 174.0649 - val_loss: 176.2658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 170.3909 - val_loss: 175.3209\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 170.1882 - val_loss: 176.0470\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 170.4658 - val_loss: 175.5888\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 169.6735 - val_loss: 174.5658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.6090 - val_loss: 174.0359\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.1891 - val_loss: 173.7398\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 168.0069 - val_loss: 173.5858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.8840 - val_loss: 173.3638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.5525 - val_loss: 173.1197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.2992 - val_loss: 172.9839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 167.1568 - val_loss: 172.8912\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.9797 - val_loss: 172.8139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.7721 - val_loss: 172.7402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.5844 - val_loss: 172.6833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.4381 - val_loss: 172.5712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.3026 - val_loss: 172.3926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.1480 - val_loss: 172.2921\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 166.0089 - val_loss: 172.2465\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.8635 - val_loss: 172.2154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.7060 - val_loss: 172.1919\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.5767 - val_loss: 172.2286\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.4496 - val_loss: 172.2980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.3409 - val_loss: 172.2975\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.2692 - val_loss: 172.3164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.1316 - val_loss: 172.3399\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 165.0570 - val_loss: 172.2097\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.9893 - val_loss: 172.2392\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.8843 - val_loss: 172.3167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.7627 - val_loss: 172.2523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.6809 - val_loss: 172.2690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.6245 - val_loss: 172.2751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.5669 - val_loss: 172.2723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.5211 - val_loss: 172.2742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.5041 - val_loss: 172.2778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.4835 - val_loss: 172.2816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.4522 - val_loss: 172.2977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.4149 - val_loss: 172.3151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.4011 - val_loss: 172.3300\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3774 - val_loss: 172.3282\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3621 - val_loss: 172.3247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3463 - val_loss: 172.3253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3430 - val_loss: 172.3261\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3395 - val_loss: 172.3267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3352 - val_loss: 172.3270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3315 - val_loss: 172.3279\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3273 - val_loss: 172.3292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3223 - val_loss: 172.3310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3189 - val_loss: 172.3330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3158 - val_loss: 172.3353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3134 - val_loss: 172.3375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.9999999949504855e-07.\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3118 - val_loss: 172.3378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3116 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3114 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3111 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3109 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3106 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3104 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3101 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3098 - val_loss: 172.3380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3095 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 2.000000023372195e-08.\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3093 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3093 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3093 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3092 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3092 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3092 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3092 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3092 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.9999999878450583e-09.\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.999999943436137e-10.\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.999999887924986e-11.\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 164.3091 - val_loss: 172.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1678bfacbe0>"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic model\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "lr=0.0002\n",
    "epochs=100\n",
    "batch_size=32\n",
    "validation_split=0.1\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "#n_timesteps, n_features, n_outputs = X_train_gru.shape[0], X_train_gru.shape[1], Y_train_gru.shape[0]\n",
    "# reshape output into [samples, timesteps, features]\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Reshape((1, X_train_gru.shape[1])))\n",
    "model.add(GRU(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "#model.add(GRU(200, activation='tanh', input_shape=(1,X_train_gru.shape[1])))\n",
    "model.add(RepeatVector(n_outputs))\n",
    "model.add(GRU(200, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "opt=optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='mae', optimizer=opt)\n",
    "\n",
    "model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            ReduceLROnPlateau(\n",
    "                verbose=1)])\n",
    "    \n",
    "    \n",
    "#model = Sequential()\n",
    "#model.add(Dense(1, input_dim=X.shape[1]))\n",
    "#model.compile(optimizer = optimizers.Adam(lr=lr), loss='mean_squared_error')\n",
    "#model.fit(X_train, y_train,\n",
    "#          validation_split=validation_split,\n",
    "#          epochs=epochs,\n",
    "#          batch_size=batch_size,\n",
    "#          shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# make a prediction to test how well the model preforms\n",
    "y_predicted = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### LZS: WE HAVE TO IMPLEMENT HERE SOME METRICS AND A LOT OF PLOTS SO THAT WE CAN SEE HOW WELL THE MODEL PERFORMS\n",
    "### LZS: AND ALSO TO BE ABLE TO FIND NEW FEATURES TO IMPROVE THE MODEL\n",
    "### LZS: AND TO EVALUATE IF THE MODEL INDEED IMPROVES WHEN ADDING MORE FEATURES/ OTHER LAYERS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted=[72.248436], Original=[84.4234122904167]\n",
      "Predicted=[86.923], Original=[862.5090558333333]\n",
      "Predicted=[88.00898], Original=[365.5664364264861]\n",
      "Predicted=[88.88442], Original=[336.159522455139]\n",
      "Predicted=[90.23452], Original=[146.23547872013904]\n",
      "Predicted=[91.5743], Original=[63.33341904555554]\n",
      "Predicted=[92.92094], Original=[498.6270929166664]\n",
      "Predicted=[94.30749], Original=[514.8907244444447]\n",
      "Predicted=[95.76746], Original=[513.2867190277781]\n",
      "Predicted=[97.24717], Original=[43.21729329622221]\n",
      "Predicted=[98.77364], Original=[35.78967036840332]\n",
      "Predicted=[100.38198], Original=[120.85732251188983]\n",
      "Predicted=[102.02639], Original=[35.80649369249831]\n",
      "Predicted=[103.73035], Original=[10.226580554208333]\n",
      "Predicted=[105.49932], Original=[91.94390274458337]\n",
      "Predicted=[107.35139], Original=[83.82949924069443]\n",
      "Predicted=[109.34038], Original=[104.96816865428814]\n",
      "Predicted=[111.46469], Original=[496.9309556388885]\n",
      "Predicted=[113.796715], Original=[151.64836002888876]\n",
      "Predicted=[116.36996], Original=[95.65714740375]\n",
      "Predicted=[119.10248], Original=[93.86623877569446]\n",
      "Predicted=[121.908165], Original=[94.59925739417781]\n",
      "Predicted=[124.78206], Original=[44.63301945527779]\n",
      "Predicted=[127.647606], Original=[514.359438333333]\n",
      "Predicted=[91.45103], Original=[695.5614265277777]\n",
      "Predicted=[144.75455], Original=[248.1164779212142]\n",
      "Predicted=[168.10944], Original=[312.5038614334025]\n",
      "Predicted=[179.6181], Original=[91.9490770348611]\n",
      "Predicted=[190.17851], Original=[895.3606036111117]\n",
      "Predicted=[191.5315], Original=[510.59596483333326]\n",
      "Predicted=[193.64905], Original=[44.677509444444446]\n",
      "Predicted=[198.29056], Original=[79.85918841194444]\n",
      "Predicted=[203.90259], Original=[88.93424712666663]\n",
      "Predicted=[209.03241], Original=[514.6614563888891]\n",
      "Predicted=[213.46404], Original=[93.41037911902775]\n",
      "Predicted=[217.20248], Original=[893.2569612499996]\n",
      "Predicted=[220.41351], Original=[42.46612747416665]\n",
      "Predicted=[223.29324], Original=[58.879927532222254]\n",
      "Predicted=[226.0008], Original=[514.6648823611116]\n",
      "Predicted=[228.64017], Original=[580.1238087499999]\n",
      "Predicted=[231.29965], Original=[515.9422194444447]\n",
      "Predicted=[234.01785], Original=[103.96548232012216]\n",
      "Predicted=[236.72702], Original=[146.7361828126389]\n",
      "Predicted=[239.2027], Original=[895.8721854166669]\n",
      "Predicted=[241.28497], Original=[35.82338299041328]\n",
      "Predicted=[242.94058], Original=[386.4051141388889]\n",
      "Predicted=[244.2438], Original=[145.55625737295844]\n",
      "Predicted=[245.29393], Original=[515.0571497222222]\n",
      "Predicted=[74.41932], Original=[496.56238333333357]\n",
      "Predicted=[92.42312], Original=[78.19499146430553]\n",
      "Predicted=[107.11201], Original=[505.1233406944443]\n",
      "Predicted=[112.572815], Original=[44.43692492704645]\n",
      "Predicted=[116.08143], Original=[35.85716158624321]\n",
      "Predicted=[119.81515], Original=[495.7129206944445]\n",
      "Predicted=[123.6515], Original=[42.153919867083324]\n",
      "Predicted=[127.57996], Original=[507.92877875000016]\n",
      "Predicted=[131.6651], Original=[460.25736093570816]\n",
      "Predicted=[135.77269], Original=[510.7527401944444]\n",
      "Predicted=[139.80707], Original=[35.79013218514318]\n",
      "Predicted=[143.60843], Original=[35.79804904354083]\n",
      "Predicted=[147.49626], Original=[174.14673291399293]\n",
      "Predicted=[151.63913], Original=[510.08210375000004]\n",
      "Predicted=[156.96167], Original=[91.51977398482158]\n",
      "Predicted=[162.50583], Original=[144.79848177791655]\n",
      "Predicted=[166.53372], Original=[73.33331787041666]\n",
      "Predicted=[170.70584], Original=[366.25058951611067]\n",
      "Predicted=[177.44983], Original=[122.14905630930562]\n",
      "Predicted=[184.42822], Original=[96.39841672277781]\n",
      "Predicted=[188.71262], Original=[283.9337174365278]\n",
      "Predicted=[191.38657], Original=[178.70997284249998]\n",
      "Predicted=[193.3959], Original=[514.3822047222222]\n",
      "Predicted=[195.12967], Original=[584.90269]\n",
      "Predicted=[70.202065], Original=[0.030574016]\n",
      "Predicted=[105.80741], Original=[112.07683295662176]\n",
      "Predicted=[111.38473], Original=[35.789868289863264]\n",
      "Predicted=[108.97467], Original=[76.96528963905556]\n",
      "Predicted=[106.63953], Original=[555.3276455797222]\n",
      "Predicted=[105.15942], Original=[350.55066811685924]\n",
      "Predicted=[103.964], Original=[0.024405601069444446]\n",
      "Predicted=[103.08838], Original=[35.85716158624321]\n",
      "Predicted=[102.465034], Original=[36.87051946114123]\n",
      "Predicted=[101.971565], Original=[65.88829828722224]\n",
      "Predicted=[101.5418], Original=[37.95143452769912]\n",
      "Predicted=[101.14812], Original=[41.889351951527786]\n",
      "Predicted=[100.780235], Original=[584.4226766666661]\n",
      "Predicted=[100.43449], Original=[513.6088468055558]\n",
      "Predicted=[100.1097], Original=[35.67225290986111]\n",
      "Predicted=[99.805664], Original=[823.5428879166674]\n",
      "Predicted=[99.52252], Original=[152.96358011249995]\n",
      "Predicted=[99.26039], Original=[92.81046331888889]\n",
      "Predicted=[99.01928], Original=[510.43465277777784]\n",
      "Predicted=[98.79905], Original=[92.40986373818056]\n",
      "Predicted=[98.59938], Original=[83.67744258236112]\n",
      "Predicted=[98.4197], Original=[35.79171555682271]\n",
      "Predicted=[98.259285], Original=[243.68925669374994]\n",
      "Predicted=[98.11726], Original=[148.91568789041673]\n",
      "Predicted=[61.164967], Original=[70.37888652443574]\n",
      "Predicted=[91.86368], Original=[161.28777767930555]\n",
      "Predicted=[102.65876], Original=[174.14673291399293]\n",
      "Predicted=[106.382545], Original=[389.78119300277797]\n",
      "Predicted=[106.40353], Original=[104.96816865428814]\n",
      "Predicted=[103.99756], Original=[82.54472654305557]\n",
      "Predicted=[100.40043], Original=[511.07796277777766]\n",
      "Predicted=[96.403725], Original=[87.8111049883195]\n",
      "Predicted=[92.72002], Original=[589.2181184722217]\n",
      "Predicted=[89.930305], Original=[749.7476970833342]\n",
      "Predicted=[88.15883], Original=[42.91901455861111]\n",
      "Predicted=[87.208176], Original=[0.11477311999999999]\n",
      "Predicted=[86.78016], Original=[125.78124407263891]\n",
      "Predicted=[86.62275], Original=[42.36284019855501]\n",
      "Predicted=[86.58442], Original=[174.14673291399293]\n",
      "Predicted=[86.5915], Original=[43.85238027499998]\n",
      "Predicted=[86.61326], Original=[495.17819820222167]\n",
      "Predicted=[86.63882], Original=[182.14450404208313]\n",
      "Predicted=[86.66526], Original=[95.34981103763894]\n",
      "Predicted=[86.6923], Original=[149.07460795291664]\n",
      "Predicted=[86.720184], Original=[195.54036961263887]\n",
      "Predicted=[86.74912], Original=[174.14673291399293]\n",
      "Predicted=[86.7793], Original=[40.11326466081489]\n",
      "Predicted=[86.81063], Original=[95.51479954222219]\n",
      "Predicted=[80.02436], Original=[90.28044263736115]\n",
      "Predicted=[110.98676], Original=[138.09061380513882]\n",
      "Predicted=[109.94782], Original=[43.062970584305546]\n",
      "Predicted=[102.2137], Original=[518.1920811111114]\n",
      "Predicted=[96.989586], Original=[43.95955084081944]\n",
      "Predicted=[94.01432], Original=[35.79065997570302]\n",
      "Predicted=[92.88445], Original=[88.09970169381941]\n",
      "Predicted=[92.78327], Original=[102.25453682584477]\n",
      "Predicted=[92.932846], Original=[35.79382671906208]\n",
      "Predicted=[93.10551], Original=[77.11005018470831]\n",
      "Predicted=[93.2718], Original=[581.4114595833327]\n",
      "Predicted=[93.46018], Original=[64.04839102282334]\n",
      "Predicted=[93.66782], Original=[83.32783584443052]\n",
      "Predicted=[93.86172], Original=[514.9764515277784]\n",
      "Predicted=[94.03921], Original=[149.54124452319428]\n",
      "Predicted=[94.200615], Original=[784.9810672222222]\n",
      "Predicted=[94.346886], Original=[248.00524819402773]\n",
      "Predicted=[94.47919], Original=[515.2922626388887]\n",
      "Predicted=[94.59876], Original=[115.91604881983328]\n",
      "Predicted=[94.70788], Original=[172.67873177833343]\n",
      "Predicted=[94.808945], Original=[192.76222828291654]\n",
      "Predicted=[94.9009], Original=[35.82338299041328]\n",
      "Predicted=[94.97866], Original=[521.0765360312238]\n",
      "Predicted=[95.04742], Original=[36.87051946114123]\n",
      "Predicted=[70.245895], Original=[41.288218174000015]\n",
      "Predicted=[104.1386], Original=[504.2622726388893]\n",
      "Predicted=[120.49491], Original=[96.46445381180558]\n",
      "Predicted=[128.64609], Original=[150.710629793611]\n",
      "Predicted=[131.74228], Original=[66.43168123290279]\n",
      "Predicted=[132.2099], Original=[312.5038614334025]\n",
      "Predicted=[130.5628], Original=[0.0]\n",
      "Predicted=[127.10076], Original=[555.8238448611114]\n",
      "Predicted=[121.78188], Original=[37.95143452769912]\n",
      "Predicted=[114.867645], Original=[297.70723592666656]\n",
      "Predicted=[107.382195], Original=[94.98075177713885]\n",
      "Predicted=[101.11254], Original=[36.87051946114123]\n",
      "Predicted=[97.49316], Original=[76.52480360943058]\n",
      "Predicted=[95.835365], Original=[88.45212456111113]\n",
      "Predicted=[95.20672], Original=[193.93297942489588]\n",
      "Predicted=[94.99278], Original=[718.2941477777777]\n",
      "Predicted=[94.91847], Original=[42.29526858486112]\n",
      "Predicted=[94.88303], Original=[104.96816865428814]\n",
      "Predicted=[94.85888], Original=[35.80649369249831]\n",
      "Predicted=[94.841606], Original=[276.5662014238889]\n",
      "Predicted=[94.83109], Original=[199.31296103708326]\n",
      "Predicted=[94.827156], Original=[147.86483918902783]\n",
      "Predicted=[94.82925], Original=[172.54473946013886]\n",
      "Predicted=[94.83651], Original=[0.0710918325]\n",
      "Predicted=[65.90143], Original=[70.37888652443574]\n",
      "Predicted=[98.12647], Original=[91.17724964497224]\n",
      "Predicted=[113.29197], Original=[82.10747907013887]\n",
      "Predicted=[122.039665], Original=[35.92471877790308]\n",
      "Predicted=[125.07887], Original=[510.15146361111107]\n",
      "Predicted=[124.99403], Original=[88.80806665819439]\n",
      "Predicted=[122.2656], Original=[516.1245395000002]\n",
      "Predicted=[117.00306], Original=[162.68479291541672]\n",
      "Predicted=[110.669106], Original=[84.11665171513883]\n",
      "Predicted=[105.304115], Original=[571.2618488888894]\n",
      "Predicted=[101.85239], Original=[554.1805633333325]\n",
      "Predicted=[99.866585], Original=[165.86186050774745]\n",
      "Predicted=[98.73036], Original=[35.79065997570302]\n",
      "Predicted=[98.04389], Original=[44.43692492704645]\n",
      "Predicted=[97.59446], Original=[243.95686713930544]\n",
      "Predicted=[97.273186], Original=[338.0182540230557]\n",
      "Predicted=[97.025925], Original=[511.05549119444487]\n",
      "Predicted=[96.82584], Original=[512.8213458333333]\n",
      "Predicted=[96.65899], Original=[79.73202652588888]\n",
      "Predicted=[96.51743], Original=[207.59292206875008]\n",
      "Predicted=[96.396095], Original=[92.33960315111113]\n",
      "Predicted=[96.291504], Original=[54.141472935416665]\n",
      "Predicted=[96.20097], Original=[102.97012169305559]\n",
      "Predicted=[96.12235], Original=[76.93219841778755]\n",
      "Predicted=[66.22344], Original=[518.7482731944447]\n",
      "Predicted=[102.33464], Original=[501.6782365277779]\n",
      "Predicted=[124.744934], Original=[190.02782214250004]\n",
      "Predicted=[137.1543], Original=[0.09548352]\n",
      "Predicted=[139.89452], Original=[90.26121866916668]\n",
      "Predicted=[138.60782], Original=[70.37888652443574]\n",
      "Predicted=[137.22432], Original=[75.13733690916668]\n",
      "Predicted=[137.22766], Original=[589.2181184722217]\n",
      "Predicted=[138.44626], Original=[92.16704267805557]\n",
      "Predicted=[140.41154], Original=[83.41035592416665]\n",
      "Predicted=[142.98996], Original=[0.0]\n",
      "Predicted=[146.19391], Original=[96.77892122444446]\n",
      "Predicted=[149.99692], Original=[42.12811148986112]\n",
      "Predicted=[154.10965], Original=[35.78973634222331]\n",
      "Predicted=[158.00504], Original=[48.397318438472226]\n",
      "Predicted=[161.08018], Original=[36.87051946114123]\n",
      "Predicted=[163.01602], Original=[164.72426218102737]\n",
      "Predicted=[163.9631], Original=[124.95120771527777]\n",
      "Predicted=[164.37297], Original=[725.3069718055555]\n",
      "Predicted=[164.57062], Original=[174.03695843780733]\n",
      "Predicted=[164.6892], Original=[589.2181184722217]\n",
      "Predicted=[164.76996], Original=[157.11181508347212]\n",
      "Predicted=[164.82623], Original=[83.66249534513892]\n",
      "Predicted=[164.86476], Original=[317.10689927194477]\n",
      "Predicted=[65.55854], Original=[84.26625544583332]\n",
      "Predicted=[97.20603], Original=[90.94972796916664]\n",
      "Predicted=[109.132195], Original=[145.48382178749992]\n",
      "Predicted=[114.35982], Original=[344.1914777256907]\n",
      "Predicted=[117.266], Original=[97.4733148451389]\n",
      "Predicted=[117.42632], Original=[219.38869054708329]\n",
      "Predicted=[113.03715], Original=[881.6537002777787]\n",
      "Predicted=[104.240685], Original=[83.35729475055552]\n",
      "Predicted=[93.34227], Original=[35.78973634222331]\n",
      "Predicted=[86.328804], Original=[35.82338299041328]\n",
      "Predicted=[83.54502], Original=[93.02631658319443]\n",
      "Predicted=[82.6489], Original=[99.35889125625002]\n",
      "Predicted=[82.24767], Original=[96.23892863472223]\n",
      "Predicted=[81.95916], Original=[178.0567398063889]\n",
      "Predicted=[81.71302], Original=[78.60880995500001]\n",
      "Predicted=[81.50056], Original=[36.05983316122281]\n",
      "Predicted=[81.32032], Original=[89.20055038433331]\n",
      "Predicted=[81.168526], Original=[513.8790808333339]\n",
      "Predicted=[81.03986], Original=[55.75106428041667]\n",
      "Predicted=[80.92935], Original=[46.812129779597214]\n",
      "Predicted=[80.83292], Original=[37.95143452769912]\n",
      "Predicted=[80.74746], Original=[94.17934543805548]\n",
      "Predicted=[80.67068], Original=[0.2007583243032325]\n",
      "Predicted=[80.60083], Original=[35.82338299041328]\n",
      "Predicted=[101.72432], Original=[91.85685875333333]\n",
      "Predicted=[143.24988], Original=[42.26891059722224]\n",
      "Predicted=[155.33232], Original=[529.3511288888889]\n",
      "Predicted=[159.60934], Original=[592.3421954444448]\n",
      "Predicted=[160.13017], Original=[42.24952249638889]\n",
      "Predicted=[159.77791], Original=[589.2181184722217]\n",
      "Predicted=[158.91728], Original=[879.4608698611113]\n",
      "Predicted=[157.6531], Original=[589.2181184722217]\n",
      "Predicted=[156.29047], Original=[53.084245459509546]\n",
      "Predicted=[155.00409], Original=[53.084245459509546]\n",
      "Predicted=[153.88202], Original=[0.24063775999999998]\n",
      "Predicted=[152.93976], Original=[390.05004228985763]\n",
      "Predicted=[152.1612], Original=[738.5607937500002]\n",
      "Predicted=[151.5159], Original=[512.0033215277778]\n",
      "Predicted=[150.97293], Original=[99.61275580902776]\n",
      "Predicted=[150.50664], Original=[287.4802583333333]\n",
      "Predicted=[150.09753], Original=[43.55596978807003]\n",
      "Predicted=[149.73154], Original=[77.12530061486113]\n",
      "Predicted=[149.39867], Original=[881.5703555555564]\n",
      "Predicted=[149.09167], Original=[86.59288183861112]\n",
      "Predicted=[148.80534], Original=[217.50313241847203]\n",
      "Predicted=[148.5358], Original=[510.14440472222196]\n",
      "Predicted=[148.2796], Original=[0.0]\n",
      "Predicted=[148.0315], Original=[35.78967036840332]\n",
      "Predicted=[85.00013], Original=[37.95143452769912]\n",
      "Predicted=[119.29746], Original=[427.17248358461694]\n",
      "Predicted=[133.56772], Original=[35.85716158624321]\n",
      "Predicted=[136.88293], Original=[92.69587064166663]\n",
      "Predicted=[135.24844], Original=[372.3110776243053]\n",
      "Predicted=[135.14828], Original=[82.06375456194446]\n",
      "Predicted=[132.34114], Original=[194.9527143215738]\n",
      "Predicted=[128.42267], Original=[207.62203622541654]\n",
      "Predicted=[125.45547], Original=[515.9180145833329]\n",
      "Predicted=[123.88514], Original=[146.71660208541664]\n",
      "Predicted=[123.378975], Original=[35.92471877790308]\n",
      "Predicted=[123.68105], Original=[0.0675136]\n",
      "Predicted=[124.628716], Original=[530.1976626388889]\n",
      "Predicted=[126.120804], Original=[659.2504209722221]\n",
      "Predicted=[128.20697], Original=[832.0021654166666]\n",
      "Predicted=[131.02975], Original=[230.42831043454308]\n",
      "Predicted=[134.41388], Original=[97.11468643527776]\n",
      "Predicted=[138.13065], Original=[394.67722737569494]\n",
      "Predicted=[141.71362], Original=[37.95143452769912]\n",
      "Predicted=[144.76703], Original=[136.05920114499997]\n",
      "Predicted=[147.08823], Original=[280.82537201177377]\n",
      "Predicted=[148.6909], Original=[205.5544446499999]\n",
      "Predicted=[149.78082], Original=[513.7464584722222]\n",
      "Predicted=[150.51743], Original=[41.2183626475346]\n",
      "Predicted=[79.279144], Original=[91.30842933581948]\n",
      "Predicted=[102.59171], Original=[401.3791379218887]\n",
      "Predicted=[110.220665], Original=[81.13229357277781]\n",
      "Predicted=[117.480415], Original=[35.78967036840332]\n",
      "Predicted=[124.38829], Original=[43.946353368000004]\n",
      "Predicted=[130.82034], Original=[36.33006192786229]\n",
      "Predicted=[139.68317], Original=[180.60187989902775]\n",
      "Predicted=[148.39972], Original=[589.2181184722217]\n",
      "Predicted=[154.03989], Original=[89.40075852958327]\n",
      "Predicted=[156.014], Original=[512.4873393055558]\n",
      "Predicted=[156.44666], Original=[437.23617075638896]\n",
      "Predicted=[156.03596], Original=[884.4395130555563]\n",
      "Predicted=[155.01974], Original=[649.0012531944441]\n",
      "Predicted=[153.6456], Original=[515.7714349999994]\n",
      "Predicted=[152.12082], Original=[95.03799284416672]\n",
      "Predicted=[150.76662], Original=[882.984241111111]\n",
      "Predicted=[149.78271], Original=[613.5406340277777]\n",
      "Predicted=[149.22083], Original=[44.43692492704645]\n",
      "Predicted=[148.97125], Original=[157.10592999444418]\n",
      "Predicted=[148.91472], Original=[35.85716158624321]\n",
      "Predicted=[148.96289], Original=[43.69962755000001]\n",
      "Predicted=[149.06377], Original=[148.20080081847232]\n",
      "Predicted=[149.19032], Original=[66.39625014986109]\n",
      "Predicted=[149.32886], Original=[520.577950972223]\n",
      "Predicted=[93.497505], Original=[90.1070807013889]\n",
      "Predicted=[142.00772], Original=[43.194564884305535]\n",
      "Predicted=[155.1118], Original=[185.9749828058333]\n",
      "Predicted=[155.14815], Original=[509.213731527778]\n",
      "Predicted=[148.44739], Original=[104.96816865428814]\n",
      "Predicted=[139.98753], Original=[900.9706904166663]\n",
      "Predicted=[135.51166], Original=[35.79171555682271]\n",
      "Predicted=[133.48131], Original=[252.05823761277782]\n",
      "Predicted=[132.2345], Original=[40.11326466081489]\n",
      "Predicted=[131.23456], Original=[91.75652818694446]\n",
      "Predicted=[130.29721], Original=[510.2768797222223]\n",
      "Predicted=[129.3671], Original=[512.7242002777776]\n",
      "Predicted=[128.47076], Original=[45.17069256676386]\n",
      "Predicted=[127.62115], Original=[89.03297181986115]\n",
      "Predicted=[126.827324], Original=[79.11918453708334]\n",
      "Predicted=[126.09808], Original=[87.99671135970836]\n",
      "Predicted=[125.473976], Original=[36.05983316122281]\n",
      "Predicted=[124.89553], Original=[153.93904609274392]\n",
      "Predicted=[124.37455], Original=[37.95143452769912]\n",
      "Predicted=[123.908455], Original=[319.74004406680535]\n",
      "Predicted=[123.49737], Original=[98.75129490638888]\n",
      "Predicted=[123.131645], Original=[506.23471972222217]\n",
      "Predicted=[122.80701], Original=[96.78072142444445]\n",
      "Predicted=[122.51918], Original=[43.663217229444435]\n",
      "Predicted=[70.02047], Original=[0.0892144]\n",
      "Predicted=[100.961754], Original=[198.42055916861108]\n",
      "Predicted=[112.96338], Original=[528.7035825000006]\n",
      "Predicted=[120.758644], Original=[129.98198312861118]\n",
      "Predicted=[124.75357], Original=[88.09801764916664]\n",
      "Predicted=[127.10949], Original=[4.950866981944442]\n",
      "Predicted=[128.60858], Original=[35.79013218514318]\n",
      "Predicted=[129.54468], Original=[173.0048033686111]\n",
      "Predicted=[130.15475], Original=[641.2865672222222]\n",
      "Predicted=[130.57587], Original=[77.59379726005909]\n",
      "Predicted=[130.83008], Original=[106.32335651861113]\n",
      "Predicted=[130.87277], Original=[180.16201697069422]\n",
      "Predicted=[130.61002], Original=[564.2985691111106]\n",
      "Predicted=[129.90704], Original=[112.721858501333]\n",
      "Predicted=[128.61545], Original=[94.50440421194442]\n",
      "Predicted=[126.30974], Original=[519.6050373611118]\n",
      "Predicted=[122.8628], Original=[147.1331510247222]\n",
      "Predicted=[118.46208], Original=[727.6472219444438]\n",
      "Predicted=[113.68566], Original=[35.79171555682271]\n",
      "Predicted=[109.29624], Original=[90.33493882361117]\n",
      "Predicted=[106.05938], Original=[511.899134444444]\n",
      "Predicted=[103.95596], Original=[798.5421468055553]\n",
      "Predicted=[102.6531], Original=[0.3472128]\n",
      "Predicted=[101.83366], Original=[150.59454511597215]\n",
      "Predicted=[76.288246], Original=[508.4770286111108]\n",
      "Predicted=[114.50927], Original=[85.3943673211111]\n",
      "Predicted=[135.40071], Original=[85.9340742070695]\n",
      "Predicted=[142.68637], Original=[40.11326466081489]\n",
      "Predicted=[138.24878], Original=[43.535885591111146]\n",
      "Predicted=[133.17812], Original=[148.51114563576]\n",
      "Predicted=[130.02016], Original=[103.42951221013885]\n",
      "Predicted=[127.36277], Original=[43.59470581777777]\n",
      "Predicted=[125.05857], Original=[40.11326466081489]\n",
      "Predicted=[123.19488], Original=[35.789868289863264]\n",
      "Predicted=[121.80669], Original=[36.33006192786229]\n",
      "Predicted=[120.86496], Original=[881.8394594444441]\n",
      "Predicted=[120.27241], Original=[229.4237991934721]\n",
      "Predicted=[119.92014], Original=[300.24228421914495]\n",
      "Predicted=[119.72253], Original=[53.084245459509546]\n",
      "Predicted=[119.62068], Original=[157.03882952144187]\n",
      "Predicted=[119.57691], Original=[82.08754003180559]\n",
      "Predicted=[119.56772], Original=[292.977216011111]\n",
      "Predicted=[119.57892], Original=[84.99902208208337]\n",
      "Predicted=[119.60186], Original=[94.96400503306947]\n",
      "Predicted=[119.63142], Original=[509.8073376111114]\n",
      "Predicted=[119.664505], Original=[258.1655387397221]\n",
      "Predicted=[119.69923], Original=[68.6748139376389]\n",
      "Predicted=[119.73447], Original=[42.61662566416673]\n",
      "Predicted=[64.675995], Original=[917.0841406944436]\n",
      "Predicted=[101.41845], Original=[216.00561259694445]\n",
      "Predicted=[113.08283], Original=[53.42217429444444]\n",
      "Predicted=[118.4248], Original=[516.8472073611111]\n",
      "Predicted=[120.89184], Original=[486.6028501388888]\n",
      "Predicted=[121.590126], Original=[510.44853125000003]\n",
      "Predicted=[121.947914], Original=[83.36565152576387]\n",
      "Predicted=[122.20488], Original=[36.05983316122281]\n",
      "Predicted=[122.36857], Original=[35.80649369249831]\n",
      "Predicted=[122.44477], Original=[99.39168347083337]\n",
      "Predicted=[122.438866], Original=[90.18466318250003]\n",
      "Predicted=[122.34939], Original=[515.926015694444]\n",
      "Predicted=[122.16709], Original=[44.43692492704645]\n",
      "Predicted=[121.87043], Original=[35.61260089083333]\n",
      "Predicted=[121.465096], Original=[45.5926678623611]\n",
      "Predicted=[120.96209], Original=[891.8706005555559]\n",
      "Predicted=[120.36264], Original=[126.0867228257688]\n",
      "Predicted=[119.675446], Original=[95.98643384333336]\n",
      "Predicted=[118.90191], Original=[720.5219268055549]\n",
      "Predicted=[118.044044], Original=[76.96131475000001]\n",
      "Predicted=[117.1043], Original=[35.79804904354083]\n",
      "Predicted=[116.08402], Original=[36.87051946114123]\n",
      "Predicted=[114.99612], Original=[41.09210675680555]\n",
      "Predicted=[113.85566], Original=[6.746743415680556]\n",
      "Predicted=[91.850716], Original=[0.39655122555555555]\n",
      "Predicted=[107.04027], Original=[45.59906551069445]\n",
      "Predicted=[109.864845], Original=[81.60037451429169]\n",
      "Predicted=[110.91514], Original=[35.78973634222331]\n",
      "Predicted=[111.9979], Original=[327.9349360791662]\n",
      "Predicted=[112.85189], Original=[603.4576293055565]\n",
      "Predicted=[113.608955], Original=[41.97783186808333]\n",
      "Predicted=[114.34937], Original=[161.87981042708336]\n",
      "Predicted=[115.0735], Original=[36.87051946114123]\n",
      "Predicted=[115.784676], Original=[511.45723402777776]\n",
      "Predicted=[116.48528], Original=[44.43692492704645]\n",
      "Predicted=[117.19262], Original=[641.9844412500006]\n",
      "Predicted=[117.916176], Original=[509.7230488888889]\n",
      "Predicted=[118.65011], Original=[104.36620558763885]\n",
      "Predicted=[119.39014], Original=[341.12941100291675]\n",
      "Predicted=[120.13514], Original=[511.96587583333366]\n",
      "Predicted=[120.88545], Original=[154.07433688748603]\n",
      "Predicted=[121.641045], Original=[82.95611232333336]\n",
      "Predicted=[122.401566], Original=[40.11326466081489]\n",
      "Predicted=[123.16645], Original=[146.06599060958334]\n",
      "Predicted=[123.93513], Original=[40.11326466081489]\n",
      "Predicted=[124.70705], Original=[83.1824019192361]\n",
      "Predicted=[125.481575], Original=[201.60922149835582]\n",
      "Predicted=[126.25823], Original=[860.2599690277766]\n",
      "Predicted=[88.14429], Original=[570.4453501388892]\n",
      "Predicted=[135.8419], Original=[908.3759630555555]\n",
      "Predicted=[154.77707], Original=[42.96279220819444]\n",
      "Predicted=[158.04298], Original=[42.485507544305555]\n",
      "Predicted=[156.39766], Original=[56.66311136693054]\n",
      "Predicted=[150.07393], Original=[35.789868289863264]\n",
      "Predicted=[146.49225], Original=[508.9250327777775]\n",
      "Predicted=[144.85123], Original=[101.31126516611108]\n",
      "Predicted=[144.10413], Original=[127.99843079666671]\n",
      "Predicted=[143.68388], Original=[103.66063153652786]\n",
      "Predicted=[143.46771], Original=[174.14673291399293]\n",
      "Predicted=[143.36075], Original=[104.96816865428814]\n",
      "Predicted=[143.32391], Original=[0.02796992]\n",
      "Predicted=[143.33113], Original=[36.33006192786229]\n",
      "Predicted=[143.36678], Original=[312.5038614334025]\n",
      "Predicted=[143.42023], Original=[154.29471448902785]\n",
      "Predicted=[143.48494], Original=[57.514420480277785]\n",
      "Predicted=[143.55664], Original=[497.37263944444413]\n",
      "Predicted=[143.63269], Original=[35.85716158624321]\n",
      "Predicted=[143.71135], Original=[512.4283552777782]\n",
      "Predicted=[143.79153], Original=[0.0217008]\n",
      "Predicted=[143.8725], Original=[573.6646241666666]\n",
      "Predicted=[143.9538], Original=[35.92471877790308]\n",
      "Predicted=[144.03516], Original=[91.31507781097217]\n",
      "Predicted=[61.221306], Original=[343.0792770967469]\n",
      "Predicted=[69.21296], Original=[35.79013218514318]\n",
      "Predicted=[70.66563], Original=[44.54717027833334]\n",
      "Predicted=[70.87239], Original=[84.4228075291666]\n",
      "Predicted=[71.28154], Original=[151.68352450916663]\n",
      "Predicted=[71.55663], Original=[35.82338299041328]\n",
      "Predicted=[71.799034], Original=[35.79171555682271]\n",
      "Predicted=[72.00616], Original=[244.41406041068072]\n",
      "Predicted=[72.20254], Original=[509.6771936111102]\n",
      "Predicted=[72.39073], Original=[45.947399023362884]\n",
      "Predicted=[72.57288], Original=[61.03755316847219]\n",
      "Predicted=[72.75013], Original=[225.62639676850665]\n",
      "Predicted=[72.92329], Original=[514.3900124999993]\n",
      "Predicted=[73.092896], Original=[44.43692492704645]\n",
      "Predicted=[73.259384], Original=[97.1864416705267]\n",
      "Predicted=[73.42302], Original=[42.1964404227778]\n",
      "Predicted=[73.58405], Original=[609.3389531944442]\n",
      "Predicted=[73.74268], Original=[0.07522944]\n",
      "Predicted=[73.89902], Original=[43.809798752933396]\n",
      "Predicted=[74.05318], Original=[82.89806150055556]\n",
      "Predicted=[74.20528], Original=[377.002004472123]\n",
      "Predicted=[74.35538], Original=[35.78973634222331]\n",
      "Predicted=[74.51038], Original=[97.08671150333336]\n",
      "Predicted=[74.666504], Original=[511.83036152777777]\n",
      "Predicted=[68.53983], Original=[392.99463205884723]\n",
      "Predicted=[86.54371], Original=[515.7855554166666]\n",
      "Predicted=[86.693344], Original=[172.9887806876388]\n",
      "Predicted=[87.809006], Original=[35.79382671906208]\n",
      "Predicted=[87.71714], Original=[132.64570319606943]\n",
      "Predicted=[87.81118], Original=[104.96816865428814]\n",
      "Predicted=[88.16819], Original=[276.8314484318057]\n",
      "Predicted=[88.72219], Original=[92.98370268708334]\n",
      "Predicted=[89.42736], Original=[539.8580250000001]\n",
      "Predicted=[90.247536], Original=[35.78967036840332]\n",
      "Predicted=[91.16372], Original=[43.095047923749995]\n",
      "Predicted=[92.157394], Original=[35.79804904354083]\n",
      "Predicted=[93.19361], Original=[173.29450481069446]\n",
      "Predicted=[94.2371], Original=[44.52755741843916]\n",
      "Predicted=[95.25713], Original=[36.33006192786229]\n",
      "Predicted=[96.21511], Original=[35.79171555682271]\n",
      "Predicted=[97.0841], Original=[514.6502906111107]\n",
      "Predicted=[97.83761], Original=[189.3268249444445]\n",
      "Predicted=[98.45587], Original=[88.65097286847218]\n",
      "Predicted=[98.928505], Original=[95.7615750341667]\n",
      "Predicted=[99.255516], Original=[213.84334691430564]\n",
      "Predicted=[99.44639], Original=[94.47960762669477]\n",
      "Predicted=[99.51766], Original=[511.30754313888866]\n",
      "Predicted=[99.489586], Original=[0.00434016]\n",
      "Predicted=[64.04596], Original=[163.0485722070832]\n",
      "Predicted=[90.89777], Original=[81.32154629083333]\n",
      "Predicted=[102.05449], Original=[204.96970723636827]\n",
      "Predicted=[102.73666], Original=[90.8723571146083]\n",
      "Predicted=[99.430275], Original=[102.0885282311389]\n",
      "Predicted=[97.40076], Original=[36.05983316122281]\n",
      "Predicted=[96.86912], Original=[488.3167886111107]\n",
      "Predicted=[97.08918], Original=[170.56063827693046]\n",
      "Predicted=[97.41046], Original=[43.028077531388895]\n",
      "Predicted=[97.63467], Original=[0.11091519999999999]\n",
      "Predicted=[97.76068], Original=[91.05541183958341]\n",
      "Predicted=[97.82556], Original=[570.9148974999998]\n",
      "Predicted=[97.85969], Original=[38.099972527500015]\n",
      "Predicted=[97.881], Original=[0.14129632]\n",
      "Predicted=[97.89843], Original=[35.79171555682271]\n",
      "Predicted=[97.915764], Original=[432.3410382645835]\n",
      "Predicted=[97.93429], Original=[35.82338299041328]\n",
      "Predicted=[97.95423], Original=[515.582383888889]\n",
      "Predicted=[97.97544], Original=[35.85716158624321]\n",
      "Predicted=[97.997734], Original=[77.4118270897222]\n",
      "Predicted=[98.02096], Original=[508.9862986111107]\n",
      "Predicted=[98.04502], Original=[511.0460972222223]\n",
      "Predicted=[98.06994], Original=[518.3223625000003]\n",
      "Predicted=[98.09568], Original=[174.14673291399293]\n",
      "Predicted=[55.79924], Original=[513.7993725]\n",
      "Predicted=[69.21767], Original=[90.86583121694447]\n",
      "Predicted=[72.6593], Original=[0.0]\n",
      "Predicted=[74.14744], Original=[571.8354304166666]\n",
      "Predicted=[75.526375], Original=[35.85716158624321]\n",
      "Predicted=[76.751076], Original=[578.284527083334]\n",
      "Predicted=[77.906136], Original=[41.98682091291667]\n",
      "Predicted=[79.03354], Original=[35.85716158624321]\n",
      "Predicted=[80.13897], Original=[0.31422075877319433]\n",
      "Predicted=[81.209595], Original=[35.78967036840332]\n",
      "Predicted=[82.136986], Original=[43.910767669027805]\n",
      "Predicted=[82.87881], Original=[528.0295416666673]\n",
      "Predicted=[83.44042], Original=[558.1304299999999]\n",
      "Predicted=[83.784035], Original=[35.78967036840332]\n",
      "Predicted=[83.89745], Original=[499.25052222222223]\n",
      "Predicted=[83.82156], Original=[117.53920444833332]\n",
      "Predicted=[83.58592], Original=[499.3906794444445]\n",
      "Predicted=[83.28019], Original=[43.0412511911111]\n",
      "Predicted=[83.10595], Original=[44.07477899637499]\n",
      "Predicted=[83.15884], Original=[225.58206111861094]\n",
      "Predicted=[83.43092], Original=[96.47121405138891]\n",
      "Predicted=[83.88697], Original=[510.39833236111144]\n",
      "Predicted=[84.4654], Original=[35.92471877790308]\n",
      "Predicted=[85.115685], Original=[341.2687461000379]\n",
      "Predicted=[66.43626], Original=[35.92471877790308]\n",
      "Predicted=[78.88878], Original=[46.81404456922135]\n",
      "Predicted=[80.69247], Original=[511.04420458333306]\n",
      "Predicted=[82.83273], Original=[95.67588946361113]\n",
      "Predicted=[84.983635], Original=[565.3037029166667]\n",
      "Predicted=[86.95989], Original=[516.4658011111107]\n",
      "Predicted=[88.85004], Original=[509.2309436111111]\n",
      "Predicted=[90.582115], Original=[458.46199377108354]\n",
      "Predicted=[92.12533], Original=[40.11326466081489]\n",
      "Predicted=[93.527336], Original=[487.37016690611097]\n",
      "Predicted=[94.81866], Original=[465.4205249525336]\n",
      "Predicted=[96.00489], Original=[19.12645740884722]\n",
      "Predicted=[97.0819], Original=[163.3501073891665]\n",
      "Predicted=[98.14447], Original=[174.14673291399293]\n",
      "Predicted=[99.21894], Original=[44.41627500944443]\n",
      "Predicted=[100.31448], Original=[90.18920949276387]\n",
      "Predicted=[101.43344], Original=[43.87017809847222]\n",
      "Predicted=[102.57422], Original=[35.789868289863264]\n",
      "Predicted=[103.74436], Original=[286.460201657595]\n",
      "Predicted=[104.94723], Original=[509.8239893055553]\n",
      "Predicted=[106.1775], Original=[496.0953843055558]\n",
      "Predicted=[107.42996], Original=[76.9692645281111]\n",
      "Predicted=[108.6995], Original=[42.57431763290277]\n",
      "Predicted=[109.98232], Original=[522.7381411111108]\n"
     ]
    }
   ],
   "source": [
    "# show the inputs, predicted outputs and real outputs\n",
    "for i in range(len(X_test[0])):\n",
    "        for j in range(len(X_test[1])):\n",
    "            print(\"Predicted=%s, Original=%s\" % (y_predicted[i][j], y_test[i][j]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3144, 1)\n",
      "(3144, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with a prediction for all datapoints for the csv\n",
    "y_predicted_all = model.predict(X_shaped)\n",
    "y_pred_all = np.reshape(y_predicted_all, (-1, 1))\n",
    "print(y_pred_all.shape)\n",
    "y_pred_all = pd.DataFrame(data=y_pred_all)\n",
    "print(TimeHour.shape)\n",
    "my_df = pd.DataFrame({'Time': TimeHour.loc[:,'TimeHour'].tolist(), 'pred_y': y_pred_all.loc[:,0].tolist()})\n",
    "my_df.to_csv('WetPredictions.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "LZS_run_code_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}