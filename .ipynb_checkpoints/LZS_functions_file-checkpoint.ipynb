{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\20182960\\\\Documents\\\\3.1 DataChallenge3\\\\jbg060_2019-2020-documented_implementations\\\\group6_DC3_documented_implementation_ORIGINALFILE\\\\Documented Implementation\\\\Code')\n",
    "\n",
    "import preprocessing as pre\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurements(path, convert_time=True):\n",
    "    \"\"\"\n",
    "    Will read all measurement data from given path and store them in separate dataframes.\n",
    "    ~~~ EXAMPLE CALL ~~~\n",
    "    flow_data, level_data = get_measurements(\"C:/mypath/RG8150\")\n",
    "    ~~~~~~~~~~~~~~~~~~~~\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    data = [pd.read_csv(path + \"/\" + i, sep = \";\") for i in files]\n",
    "    data =  pd.concat(data, sort = False, ignore_index = True)\n",
    "    \n",
    "    data[\"RG_ID\"] = data[\"Tagname\"].str.slice(9,13).astype(int)\n",
    "    data[\"Value\"] = data[\"Value\"].str.replace(\",\", \".\").astype(float)\n",
    "    data[\"DataQuality\"] = (data[\"DataQuality\"] == \"Good\").astype(int)\n",
    "    if convert_time == True:\n",
    "        data[\"TimeStamp\"] = pd.to_datetime(data[\"TimeStamp\"], format=\"%d-%m-%Y %H:%M:%S\")\n",
    "        \n",
    "    data = data[[\"Tagname\", \"RG_ID\", \"TimeStamp\", \"Value\", \"DataQuality\"]]\n",
    "    \n",
    "    flow_data = data[data[\"Tagname\"].str.contains(\"Debietmeting\")].reset_index(drop = True)\n",
    "    level_data = data[data[\"Tagname\"].str.contains(\"Niveaumeting\")].reset_index(drop = True)\n",
    "    \n",
    "    flow_data.drop(\"Tagname\", axis=1, inplace=True)\n",
    "    level_data.drop(\"Tagname\", axis=1, inplace=True)\n",
    "    \n",
    "    return flow_data, level_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictor_columns(data):\n",
    "    \"\"\"\n",
    "    Will return predictive variables given a data-set with the 'TimeHour' column.\n",
    "    'TimeHour' can be created by applying the .replace() method on the 'TimeStamp'\n",
    "    column. The following variables will be added:\n",
    "\n",
    "    NAME ~~~~~~~~~~~~~~~ COLUMN ~~~~~~~ FORMAT\n",
    "    Hour of the day      hour_XX        Dummy, binary\n",
    "    Month of the year    month_XX       Dummy, binary\n",
    "    Holiday              is_holiday     Binary\n",
    "\n",
    "    Holiday is based on all holidays in the Netherlands in 2018, 2019, 2020.\n",
    "    \"\"\"\n",
    "    # Fetch holidays of given period\n",
    "    NL_holidays = [i[0] for i in holidays.Netherlands(years = [2018, 2019, 2020]).items()]\n",
    "    \n",
    "    # Check each date whether in holidays\n",
    "    is_holiday = data[\"TimeHour\"].astype('datetime64[ns]').apply(lambda i: i.date() in NL_holidays).astype(int)\n",
    "\n",
    "    # Create dummies for hour of day and month of year\n",
    "    hour_dummies = pd.get_dummies(data[\"TimeHour\"].astype('datetime64[ns]').apply(lambda i: i.hour), prefix=\"hour\")\n",
    "    month_dummies = pd.get_dummies(data[\"TimeHour\"].astype('datetime64[ns]').apply(lambda i: i.month), prefix=\"month\")\n",
    "   \n",
    "    # Concatenate and add constant/intercept\n",
    "    X = pd.concat([hour_dummies, month_dummies, is_holiday], axis=1)\n",
    "    X[\"Constant\"] = 1\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rain_predictions_file (hirlam_filelocation):\n",
    "    \n",
    "    hirlam2018 = pd.read_csv(hirlam_filelocation + '2018_hirlam_predictions.csv', sep=';')\n",
    "    hirlam2019 = pd.read_csv(hirlam_filelocation + '2019_hirlam_predictions.csv', sep=',')\n",
    "    hirlam2020 = pd.read_csv(hirlam_filelocation + '2020_hirlam_predictions.csv', sep=',')\n",
    "    \n",
    "    hirlamList = [hirlam2018, hirlam2019, hirlam2020]\n",
    "    \n",
    "    LZS_rain_pred = pd.DataFrame(columns=['Time', 'Prediction'])\n",
    "    \n",
    "    for hirlam in hirlamList: \n",
    "\n",
    "        start = 12\n",
    "        while start < len(hirlam):\n",
    "            LZS_rain_pred = LZS_rain_pred.append(hirlam[start:start+6][['Time', 'Prediction']])\n",
    "            start=start+49\n",
    "\n",
    "\n",
    "    LZS_rain_pred = LZS_rain_pred.rename(columns={\"Time\": \"TimeHour\"})\n",
    "    LZS_rain_pred = LZS_rain_pred.reset_index()\n",
    "    \n",
    "    return(LZS_rain_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(rain_prediction, flow_data, level_data, imputation = \"simple\"):\n",
    "\n",
    "\n",
    "    # LZS: no rain grid needed, since the rain predictions are the same for the whole area.\n",
    "\n",
    "    # Omit minor data defficiencies\n",
    "    flow_data = pre.clean_mes_data(flow_data, convert_timestamp=False)\n",
    "    level_data = pre.clean_mes_data(level_data, convert_timestamp=False)\n",
    "\n",
    "    # Merges flow and level on timestamps, as normal flow data is biased\n",
    "    # given no measurements are made when there is no flow.\n",
    "    flow_data, level_data = pre.merge_flow_level(flow_data, level_data)\n",
    "\n",
    "    # Can perform simple imputation or LM-imputation\n",
    "    if imputation == \"simple\":\n",
    "        flow_data = pre.fill_flow(flow_data)\n",
    "    elif imputation == \"complex\":\n",
    "        flow_data = data_imputation.fill_flow(flow_data, level_data)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Groups flow by hour\n",
    "    flow_data_by_hour = pre.flow_by_hour(flow_data)\n",
    "      \n",
    "    flow_data_by_hour['TimeHour'] = flow_data_by_hour['TimeHour'].astype(str)\n",
    "    rain_prediction['TimeHour'] = rain_prediction['TimeHour'].astype(str)\n",
    "\n",
    "    result = pd.merge(flow_data_by_hour, rain_prediction, on='TimeHour', how='inner')\n",
    "\n",
    "    # Concatenate grid data and other variables\n",
    "    X = add_predictor_columns(result).values\n",
    "\n",
    "    X = np.concatenate((result[['Prediction']], X), axis=1)\n",
    "   \n",
    "    # Select dependent variable\n",
    "    y = result[\"Flow\"].values    \n",
    "    TimeHour = result['TimeHour']\n",
    "    \n",
    "    return(X,y,TimeHour)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
